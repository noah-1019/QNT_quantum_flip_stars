{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e33d66",
   "metadata": {},
   "source": [
    "# Train Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ceaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable-Baselines3 and Gymnasium Implementation\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "\n",
    "# Add this to your imports cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import our functions\n",
    "from main import updated_fisher_information, reward_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c148b14",
   "metadata": {},
   "source": [
    "### Create Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a93f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumNetworkEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Gymnasium Environment for Quantum Network Protocol Selection\n",
    "    \n",
    "    The agent receives theta values and current Fisher information, then selects\n",
    "    protocols to maximize cumulative reward over multiple steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 4}\n",
    "    \n",
    "    def __init__(self, max_steps=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.render_mode = render_mode\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Action space: Combined root (3 options) and protocol (6 options) selection\n",
    "        # Total actions = 3 roots × 6 protocols = 18 possible actions\n",
    "        # Action encoding: action = root * 6 + protocol\n",
    "        # Where root ∈ {0, 1, 2} and protocol ∈ {0, 1, 2, 3, 4, 5}\n",
    "        self.action_space = spaces.Discrete(18)\n",
    "        \n",
    "        # Observation space: Flattened Fisher matrix (3x3 = 9 values) + theta parameters (3 values)\n",
    "        # Total observation size: 9 + 3 = 12 values\n",
    "        # Fisher matrix values: -10 to 100, Theta values: 0.05 to 0.45\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-10.0] * 9 + [0.05] * 3, dtype=np.float32),  # 9 Fisher + 3 theta\n",
    "            high=np.array([100.0] * 9 + [0.45] * 3, dtype=np.float32),  # 9 Fisher + 3 theta\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.theta = None\n",
    "        self.fisher_matrix = None\n",
    "        self.total_reward = 0\n",
    "        self.episode_history = []\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.total_reward = 0\n",
    "        self.episode_history = []\n",
    "        \n",
    "        # Initialize theta values randomly\n",
    "        self.theta = [np.random.uniform(0.05, 0.45) for _ in range(3)]\n",
    "        \n",
    "        # Initialize Fisher matrix to zeros (3x3)\n",
    "        self.fisher_matrix = np.zeros((3, 3), dtype=np.float32)\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        if not self.action_space.contains(action):\n",
    "            raise ValueError(f\"Invalid action: {action}. Must be in {self.action_space}\")\n",
    "        \n",
    "        # Decode the combined action into root and protocol\n",
    "        # action = root * 6 + protocol\n",
    "        root = action // 6  # Integer division to get root (0, 1, or 2)\n",
    "        protocol = action % 6  # Modulo to get protocol (0, 1, 2, 3, 4, 5)\n",
    "        \n",
    "        # Store old fisher matrix for analysis\n",
    "        old_fisher_matrix = self.fisher_matrix.copy()\n",
    "        \n",
    "        try:\n",
    "            # Update Fisher information using the selected root and protocol\n",
    "            new_fisher_matrix = updated_fisher_information(root, protocol, self.theta)\n",
    "            \n",
    "            # Add new contribution to existing Fisher matrix\n",
    "            self.fisher_matrix += new_fisher_matrix\n",
    "            \n",
    "            # Calculate reward using the updated Fisher matrix\n",
    "            reward = reward_function(self.fisher_matrix)\n",
    "            \n",
    "        except ZeroDivisionError:\n",
    "            # Handle edge case where theta values cause division by zero\n",
    "            reward = -10.0  # Penalty for invalid state\n",
    "            new_fisher_matrix = np.zeros((3, 3), dtype=np.float32)\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        terminated = self.step_count >= self.max_steps\n",
    "        truncated = False  # We don't have time limits beyond max_steps\n",
    "        \n",
    "\n",
    "        # Store step information for analysis\n",
    "        step_info = {\n",
    "            'step': self.step_count,\n",
    "            'action': action,\n",
    "            'root': root,\n",
    "            'protocol': protocol,\n",
    "            'reward': reward,\n",
    "            'fisher_matrix': self.fisher_matrix.copy(),\n",
    "            'fisher_contribution': new_fisher_matrix.copy(),\n",
    "            'theta': self.theta.copy()\n",
    "        }\n",
    "        self.episode_history.append(step_info)\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        info.update(step_info)\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation: flattened Fisher matrix (9 values) + theta parameters (3 values)\"\"\"\n",
    "        # Flatten the 3x3 Fisher matrix to a 1D array\n",
    "        fisher_flat = self.fisher_matrix.flatten()\n",
    "        \n",
    "        # Combine flattened Fisher matrix with theta parameters\n",
    "        obs = np.concatenate([fisher_flat, self.theta], dtype=np.float32)\n",
    "        \n",
    "        # Clip to ensure it's within observation space bounds\n",
    "        return np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"Get additional information about the current state\"\"\"\n",
    "        return {\n",
    "            'step_count': self.step_count,\n",
    "            'total_reward': self.total_reward,\n",
    "            'theta': self.theta.copy(),\n",
    "            'fisher_matrix': self.fisher_matrix.copy(),\n",
    "            'fisher_diagonal': np.diag(self.fisher_matrix).copy()\n",
    "        }\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        \"\"\"Decode combined action into root and protocol\"\"\"\n",
    "        root = action // 6\n",
    "        protocol = action % 6\n",
    "        return root, protocol\n",
    "    \n",
    "    def encode_action(self, root, protocol):\n",
    "        \"\"\"Encode root and protocol into combined action\"\"\"\n",
    "        return root * 6 + protocol\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment (optional)\"\"\"\n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"Step: {self.step_count}/{self.max_steps}\")\n",
    "            print(f\"Theta: {[f'{x:.3f}' for x in self.theta]}\")\n",
    "            print(f\"Fisher Matrix Diagonal: {[f'{x:.3f}' for x in np.diag(self.fisher_matrix)]}\")\n",
    "            print(f\"Fisher Matrix Trace: {np.trace(self.fisher_matrix):.3f}\")\n",
    "            print(f\"Total Reward: {self.total_reward:.3f}\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up environment\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca781a12",
   "metadata": {},
   "source": [
    "### Test Enviornemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d25d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Quantum Network Environment with Root + Protocol Selection...\n",
      "Checking environment...\n",
      "Environment check failed: The reward returned by `step()` must be a float\n",
      "\n",
      "Testing environment reset...\n",
      "Initial observation shape: (12,)\n",
      "Initial observation: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.19981605 0.43028572 0.34279758]\n",
      "Action space: Discrete(18) (18 total actions)\n",
      "Observation space: Box([-10.   -10.   -10.   -10.   -10.   -10.   -10.   -10.   -10.     0.05\n",
      "   0.05   0.05], [100.   100.   100.   100.   100.   100.   100.   100.   100.     0.45\n",
      "   0.45   0.45], (12,), float32)\n",
      "\n",
      "Action Encoding Examples:\n",
      "Action = Root * 6 + Protocol\n",
      "Root 0, Protocol 0 → Action 0\n",
      "Root 0, Protocol 1 → Action 1\n",
      "Root 0, Protocol 2 → Action 2\n",
      "Root 0, Protocol 3 → Action 3\n",
      "Root 0, Protocol 4 → Action 4\n",
      "Root 0, Protocol 5 → Action 5\n",
      "Root 1, Protocol 0 → Action 6\n",
      "Root 1, Protocol 1 → Action 7\n",
      "Root 1, Protocol 2 → Action 8\n",
      "Root 1, Protocol 3 → Action 9\n",
      "Root 1, Protocol 4 → Action 10\n",
      "Root 1, Protocol 5 → Action 11\n",
      "Root 2, Protocol 0 → Action 12\n",
      "Root 2, Protocol 1 → Action 13\n",
      "Root 2, Protocol 2 → Action 14\n",
      "Root 2, Protocol 3 → Action 15\n",
      "Root 2, Protocol 4 → Action 16\n",
      "Root 2, Protocol 5 → Action 17\n",
      "\n",
      "Testing random steps with root and protocol details...\n",
      "Step 1: Action=1 (Root=0, Protocol=1), Reward=1.140, Done=False\n",
      "Step 2: Action=4 (Root=0, Protocol=4), Reward=0.286, Done=False\n",
      "Step 3: Action=3 (Root=0, Protocol=3), Reward=1.352, Done=False\n",
      "Environment test completed successfully! ✓\n"
     ]
    }
   ],
   "source": [
    "# Create and test the environment with updated action space\n",
    "print(\"Creating Quantum Network Environment with Root + Protocol Selection...\")\n",
    "env = QuantumNetworkEnv(max_steps=10)\n",
    "\n",
    "# Check if the environment follows Gymnasium API\n",
    "print(\"Checking environment...\")\n",
    "try:\n",
    "    check_env(env, warn=True)\n",
    "    print(\"Environment check passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment check failed: {e}\")\n",
    "\n",
    "# Test the environment\n",
    "print(\"\\nTesting environment reset...\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Initial observation: {obs}\")\n",
    "print(f\"Action space: {env.action_space} (18 total actions)\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Demonstrate action encoding\n",
    "print(\"\\nAction Encoding Examples:\")\n",
    "print(\"Action = Root * 6 + Protocol\")\n",
    "for root in range(3):\n",
    "    for protocol in range(6):\n",
    "        encoded_action = env.encode_action(root, protocol)\n",
    "        decoded_root, decoded_protocol = env.decode_action(encoded_action)\n",
    "        print(f\"Root {root}, Protocol {protocol} → Action {encoded_action}\")\n",
    "print()\n",
    "\n",
    "# Test a few random steps with detailed output\n",
    "print(\"Testing random steps with root and protocol details...\")\n",
    "for i in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    root, protocol = env.decode_action(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action} (Root={root}, Protocol={protocol}), Reward={reward:.3f}, Done={terminated}\")\n",
    "    \n",
    "env.close()\n",
    "print(\"Environment test completed successfully! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206407ac",
   "metadata": {},
   "source": [
    "### Create RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5926ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training environment...\n",
      "Creating PPO model...\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Model created successfully!\n",
      "Policy architecture: ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=18, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Training setup completed! Ready to train the agent.\n"
     ]
    }
   ],
   "source": [
    "# Train a PPO agent using Stable-Baselines3\n",
    "\n",
    "print(\"Setting up training environment...\")\n",
    "\n",
    "# Create training and evaluation environments\n",
    "train_env = QuantumNetworkEnv(max_steps=20)\n",
    "eval_env = QuantumNetworkEnv(max_steps=20)\n",
    "\n",
    "# Wrap environments with Monitor for logging\n",
    "train_env = Monitor(train_env)\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Create the PPO model\n",
    "print(\"Creating PPO model...\")\n",
    "# Better hyperparameters for improved learning\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    verbose=1,\n",
    "    learning_rate=5e-5,  # Reduced learning rate\n",
    "    n_steps=4096,        # More steps per update\n",
    "    batch_size=256,      # Larger batch size\n",
    "    n_epochs=20,         # More epochs per update\n",
    "    gamma=0.95,          # Slightly reduced discount factor\n",
    "    gae_lambda=0.9,      # Reduced GAE lambda\n",
    "    clip_range=0.1,      # Tighter clipping\n",
    "    ent_coef=0.05,       # More exploration\n",
    "    vf_coef=0.5,         # Value function coefficient\n",
    "    max_grad_norm=0.5,   # Gradient clipping\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[128, 128, 64],  # Deeper network\n",
    "        activation_fn=torch.nn.ReLU\n",
    "    ),\n",
    "    device=\"cpu\",\n",
    "    tensorboard_log=\"./ppo_protocol_picker_logs/\"\n",
    ")\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Policy architecture: {model.policy}\")\n",
    "\n",
    "# Create evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_protocol_picker/\",\n",
    "    log_path=\"./protocol_picker_eval_logs/\",\n",
    "    eval_freq=1000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training setup completed! Ready to train the agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511db9a",
   "metadata": {},
   "source": [
    "### Train the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785260c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 10,000 timesteps (adjust as needed)\n",
      "Logging to ./ppo_protocol_picker_logs/PPO_QuantumNetwork_4\n",
      "Eval num_timesteps=216, episode_reward=46.84 +/- 12.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216, episode_reward=46.84 +/- 12.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1216, episode_reward=35.10 +/- 7.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1216, episode_reward=35.10 +/- 7.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2216, episode_reward=38.49 +/- 11.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2216, episode_reward=38.49 +/- 11.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3216, episode_reward=39.22 +/- 11.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3216, episode_reward=39.22 +/- 11.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3216     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 5139     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 5139     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4216, episode_reward=35.58 +/- 9.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014488355 |\n",
      "|    clip_fraction        | 0.0633       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 8.89         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4216, episode_reward=35.58 +/- 9.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014488355 |\n",
      "|    clip_fraction        | 0.0633       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 8.89         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5216, episode_reward=46.91 +/- 12.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5216, episode_reward=46.91 +/- 12.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6216, episode_reward=49.50 +/- 6.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6216, episode_reward=49.50 +/- 6.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7216, episode_reward=39.00 +/- 14.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7216, episode_reward=39.00 +/- 14.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7216     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 4088     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 4088     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8216, episode_reward=33.22 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014281599 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.657        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.81         |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00465     |\n",
      "|    value_loss           | 16.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8216, episode_reward=33.22 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014281599 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.657        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.81         |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00465     |\n",
      "|    value_loss           | 16.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9216, episode_reward=31.17 +/- 8.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9216, episode_reward=31.17 +/- 8.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10216, episode_reward=34.60 +/- 13.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10216, episode_reward=34.60 +/- 13.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11216, episode_reward=33.07 +/- 2.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11216, episode_reward=33.07 +/- 2.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12216, episode_reward=37.49 +/- 10.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12216, episode_reward=37.49 +/- 10.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3815     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3815     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13216, episode_reward=35.77 +/- 16.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014929022 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.78         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13216, episode_reward=35.77 +/- 16.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014929022 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.78         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14216, episode_reward=40.15 +/- 12.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14216, episode_reward=40.15 +/- 12.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15216, episode_reward=37.99 +/- 14.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15216, episode_reward=37.99 +/- 14.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16216, episode_reward=45.09 +/- 10.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16216, episode_reward=45.09 +/- 10.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3706     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3706     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17216, episode_reward=30.45 +/- 7.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014994405 |\n",
      "|    clip_fraction        | 0.0719       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.699        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 7.64         |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00611     |\n",
      "|    value_loss           | 15.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17216, episode_reward=30.45 +/- 7.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014994405 |\n",
      "|    clip_fraction        | 0.0719       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.699        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 7.64         |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00611     |\n",
      "|    value_loss           | 15.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18216, episode_reward=34.62 +/- 7.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18216, episode_reward=34.62 +/- 7.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19216, episode_reward=27.62 +/- 3.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19216, episode_reward=27.62 +/- 3.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20216, episode_reward=34.92 +/- 10.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20216, episode_reward=34.92 +/- 10.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3637     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3637     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21216, episode_reward=47.68 +/- 9.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015100653 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.78         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00579     |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21216, episode_reward=47.68 +/- 9.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015100653 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.78         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00579     |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22216, episode_reward=44.54 +/- 15.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22216, episode_reward=44.54 +/- 15.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23216, episode_reward=32.68 +/- 11.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23216, episode_reward=32.68 +/- 11.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24216, episode_reward=47.40 +/- 10.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24216, episode_reward=47.40 +/- 10.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3600     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3600     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25216, episode_reward=45.42 +/- 15.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 45.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015923965 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.717        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 7.51         |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00578     |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25216, episode_reward=45.42 +/- 15.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 45.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015923965 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.717        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 7.51         |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00578     |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26216, episode_reward=33.66 +/- 12.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26216, episode_reward=33.66 +/- 12.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27216, episode_reward=47.07 +/- 12.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27216, episode_reward=47.07 +/- 12.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28216, episode_reward=49.92 +/- 10.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28216, episode_reward=49.92 +/- 10.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3576     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3576     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29216, episode_reward=55.14 +/- 4.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 55.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015661006 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.721        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.41         |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00431     |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=29216, episode_reward=55.14 +/- 4.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 55.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015661006 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.721        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.41         |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00431     |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30216, episode_reward=48.56 +/- 9.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30216, episode_reward=48.56 +/- 9.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31216, episode_reward=49.46 +/- 5.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31216, episode_reward=49.46 +/- 5.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32216, episode_reward=52.31 +/- 13.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32216, episode_reward=52.31 +/- 13.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3555     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3555     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33216, episode_reward=46.98 +/- 13.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012493716 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.1          |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 12           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33216, episode_reward=46.98 +/- 13.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012493716 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.1          |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 12           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=34216, episode_reward=53.19 +/- 11.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 53.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34216, episode_reward=53.19 +/- 11.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 53.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35216, episode_reward=40.01 +/- 13.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35216, episode_reward=40.01 +/- 13.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36216, episode_reward=41.01 +/- 12.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36216, episode_reward=41.01 +/- 12.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3541     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3541     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37216, episode_reward=34.77 +/- 10.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015713256 |\n",
      "|    clip_fraction        | 0.0726       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.49         |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00698     |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=37216, episode_reward=34.77 +/- 10.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015713256 |\n",
      "|    clip_fraction        | 0.0726       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.49         |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00698     |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38216, episode_reward=50.56 +/- 10.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 50.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38216, episode_reward=50.56 +/- 10.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 50.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39216, episode_reward=45.11 +/- 18.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39216, episode_reward=45.11 +/- 18.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40216, episode_reward=38.63 +/- 14.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40216, episode_reward=38.63 +/- 14.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3529     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3529     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41216, episode_reward=30.37 +/- 11.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013151339 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.1          |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00607     |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=41216, episode_reward=30.37 +/- 11.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013151339 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.1          |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00607     |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42216, episode_reward=37.75 +/- 11.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42216, episode_reward=37.75 +/- 11.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43216, episode_reward=44.39 +/- 11.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43216, episode_reward=44.39 +/- 11.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44216, episode_reward=44.83 +/- 9.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44216, episode_reward=44.83 +/- 9.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3522     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3522     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45216, episode_reward=46.10 +/- 13.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 46.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012980365 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.86         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45216, episode_reward=46.10 +/- 13.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 46.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012980365 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.86         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=46216, episode_reward=44.57 +/- 14.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46216, episode_reward=44.57 +/- 14.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47216, episode_reward=31.64 +/- 10.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47216, episode_reward=31.64 +/- 10.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48216, episode_reward=32.06 +/- 13.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48216, episode_reward=32.06 +/- 13.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3515     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3515     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49216, episode_reward=38.44 +/- 14.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015493021 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.37         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00656     |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49216, episode_reward=38.44 +/- 14.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015493021 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.37         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00656     |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50216, episode_reward=37.04 +/- 9.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50216, episode_reward=37.04 +/- 9.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51216, episode_reward=36.10 +/- 10.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51216, episode_reward=36.10 +/- 10.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52216, episode_reward=41.10 +/- 8.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52216, episode_reward=41.10 +/- 8.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53216, episode_reward=33.79 +/- 11.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53216, episode_reward=33.79 +/- 11.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3505     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3505     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54216, episode_reward=35.21 +/- 10.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012980972 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.15         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0058      |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=54216, episode_reward=35.21 +/- 10.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012980972 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.15         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0058      |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55216, episode_reward=31.41 +/- 12.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55216, episode_reward=31.41 +/- 12.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56216, episode_reward=36.37 +/- 9.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56216, episode_reward=36.37 +/- 9.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57216, episode_reward=46.41 +/- 11.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57216, episode_reward=46.41 +/- 11.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3500     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3500     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58216, episode_reward=30.35 +/- 6.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011424215 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.58         |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.00521     |\n",
      "|    value_loss           | 9.96         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=58216, episode_reward=30.35 +/- 6.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011424215 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 6.58         |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.00521     |\n",
      "|    value_loss           | 9.96         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=59216, episode_reward=38.61 +/- 7.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59216, episode_reward=38.61 +/- 7.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60216, episode_reward=38.64 +/- 9.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60216, episode_reward=38.64 +/- 9.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61216, episode_reward=40.42 +/- 8.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61216, episode_reward=40.42 +/- 8.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3493     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3493     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62216, episode_reward=40.04 +/- 8.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 62216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017017818 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.93         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00712     |\n",
      "|    value_loss           | 9.16         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=62216, episode_reward=40.04 +/- 8.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 62216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017017818 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.93         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00712     |\n",
      "|    value_loss           | 9.16         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=63216, episode_reward=35.79 +/- 7.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63216, episode_reward=35.79 +/- 7.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64216, episode_reward=29.74 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64216, episode_reward=29.74 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65216, episode_reward=42.77 +/- 6.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65216, episode_reward=42.77 +/- 6.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3490     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3490     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66216, episode_reward=38.17 +/- 6.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 66216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015767002 |\n",
      "|    clip_fraction        | 0.0748       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.39         |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 8.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=66216, episode_reward=38.17 +/- 6.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 66216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015767002 |\n",
      "|    clip_fraction        | 0.0748       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.39         |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 8.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=67216, episode_reward=35.84 +/- 9.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67216, episode_reward=35.84 +/- 9.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68216, episode_reward=34.19 +/- 10.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68216, episode_reward=34.19 +/- 10.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69216, episode_reward=32.53 +/- 6.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69216, episode_reward=32.53 +/- 6.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3486     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3486     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70216, episode_reward=34.09 +/- 3.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015024517 |\n",
      "|    clip_fraction        | 0.0569       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.75         |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    value_loss           | 10.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=70216, episode_reward=34.09 +/- 3.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015024517 |\n",
      "|    clip_fraction        | 0.0569       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.75         |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    value_loss           | 10.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=71216, episode_reward=38.18 +/- 9.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71216, episode_reward=38.18 +/- 9.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72216, episode_reward=39.75 +/- 8.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72216, episode_reward=39.75 +/- 8.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73216, episode_reward=27.90 +/- 5.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73216, episode_reward=27.90 +/- 5.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3483     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3483     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74216, episode_reward=44.74 +/- 8.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 74216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014811114 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.63         |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00589     |\n",
      "|    value_loss           | 9.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=74216, episode_reward=44.74 +/- 8.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 74216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014811114 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.63         |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00589     |\n",
      "|    value_loss           | 9.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=75216, episode_reward=43.36 +/- 10.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75216, episode_reward=43.36 +/- 10.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76216, episode_reward=42.46 +/- 13.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76216, episode_reward=42.46 +/- 13.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77216, episode_reward=40.56 +/- 10.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77216, episode_reward=40.56 +/- 10.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3481     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3481     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78216, episode_reward=47.03 +/- 11.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011914966 |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.87         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 9.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=78216, episode_reward=47.03 +/- 11.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011914966 |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.87         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 9.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=79216, episode_reward=41.44 +/- 13.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79216, episode_reward=41.44 +/- 13.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80216, episode_reward=44.54 +/- 11.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80216, episode_reward=44.54 +/- 11.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81216, episode_reward=37.77 +/- 8.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81216, episode_reward=37.77 +/- 8.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3478     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3478     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82216, episode_reward=41.98 +/- 10.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 82216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014623743 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.58         |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    value_loss           | 8.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=82216, episode_reward=41.98 +/- 10.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 82216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014623743 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.58         |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    value_loss           | 8.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=83216, episode_reward=44.23 +/- 10.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83216, episode_reward=44.23 +/- 10.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84216, episode_reward=45.19 +/- 15.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84216, episode_reward=45.19 +/- 15.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85216, episode_reward=44.22 +/- 16.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85216, episode_reward=44.22 +/- 16.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3477     |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3477     |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86216, episode_reward=53.17 +/- 11.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 53.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 86216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014582099 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.24         |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.00606     |\n",
      "|    value_loss           | 9.2          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=86216, episode_reward=53.17 +/- 11.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 53.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 86216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014582099 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.24         |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.00606     |\n",
      "|    value_loss           | 9.2          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=87216, episode_reward=52.28 +/- 3.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87216, episode_reward=52.28 +/- 3.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88216, episode_reward=51.50 +/- 10.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 51.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88216, episode_reward=51.50 +/- 10.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 51.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89216, episode_reward=53.27 +/- 11.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 53.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89216, episode_reward=53.27 +/- 11.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 53.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3468     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3468     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90216, episode_reward=58.98 +/- 3.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 59           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012962532 |\n",
      "|    clip_fraction        | 0.0587       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.59         |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00565     |\n",
      "|    value_loss           | 9.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90216, episode_reward=58.98 +/- 3.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 59           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012962532 |\n",
      "|    clip_fraction        | 0.0587       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.59         |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00565     |\n",
      "|    value_loss           | 9.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=91216, episode_reward=47.45 +/- 14.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91216, episode_reward=47.45 +/- 14.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92216, episode_reward=50.41 +/- 10.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 50.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92216, episode_reward=50.41 +/- 10.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 50.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93216, episode_reward=55.16 +/- 5.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 55.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93216, episode_reward=55.16 +/- 5.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 55.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3467     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3467     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94216, episode_reward=56.55 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 56.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 94216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016249305 |\n",
      "|    clip_fraction        | 0.0738       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.52         |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.00739     |\n",
      "|    value_loss           | 8.9          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=94216, episode_reward=56.55 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 56.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 94216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016249305 |\n",
      "|    clip_fraction        | 0.0738       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.52         |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.00739     |\n",
      "|    value_loss           | 8.9          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=95216, episode_reward=46.36 +/- 12.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95216, episode_reward=46.36 +/- 12.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96216, episode_reward=48.93 +/- 12.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96216, episode_reward=48.93 +/- 12.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97216, episode_reward=51.91 +/- 4.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 51.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97216, episode_reward=51.91 +/- 4.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 51.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98216, episode_reward=52.13 +/- 13.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98216    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98216, episode_reward=52.13 +/- 13.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98216    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3463     |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3463     |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99216, episode_reward=41.19 +/- 18.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013376595 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.54         |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    value_loss           | 9.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=99216, episode_reward=41.19 +/- 18.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013376595 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.54         |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    value_loss           | 9.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100216, episode_reward=56.89 +/- 6.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 56.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100216, episode_reward=56.89 +/- 6.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 56.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101216, episode_reward=47.08 +/- 11.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101216, episode_reward=47.08 +/- 11.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 47.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102216, episode_reward=57.55 +/- 3.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 57.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102216, episode_reward=57.55 +/- 3.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 57.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3461     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3461     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103216, episode_reward=44.32 +/- 8.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 103216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013531952 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.89         |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 8.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=103216, episode_reward=44.32 +/- 8.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 103216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013531952 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.89         |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 8.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104216, episode_reward=52.63 +/- 6.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104216, episode_reward=52.63 +/- 6.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105216, episode_reward=45.79 +/- 14.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105216, episode_reward=45.79 +/- 14.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106216, episode_reward=48.55 +/- 12.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106216, episode_reward=48.55 +/- 12.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3459     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3459     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107216, episode_reward=47.59 +/- 14.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 107216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015325884 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.35         |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 8.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=107216, episode_reward=47.59 +/- 14.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 107216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015325884 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.35         |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 8.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108216, episode_reward=54.49 +/- 2.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 54.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108216, episode_reward=54.49 +/- 2.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 54.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109216, episode_reward=51.87 +/- 5.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 51.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109216, episode_reward=51.87 +/- 5.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 51.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110216, episode_reward=46.63 +/- 8.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110216, episode_reward=46.63 +/- 8.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3458     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3458     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111216, episode_reward=51.88 +/- 8.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 51.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 111216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00165559 |\n",
      "|    clip_fraction        | 0.0659     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.82      |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 4.64       |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.00672   |\n",
      "|    value_loss           | 9.34       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=111216, episode_reward=51.88 +/- 8.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 51.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 111216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00165559 |\n",
      "|    clip_fraction        | 0.0659     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.82      |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 4.64       |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.00672   |\n",
      "|    value_loss           | 9.34       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=112216, episode_reward=39.84 +/- 12.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112216, episode_reward=39.84 +/- 12.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113216, episode_reward=50.89 +/- 7.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 50.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113216, episode_reward=50.89 +/- 7.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 50.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114216, episode_reward=46.03 +/- 5.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114216, episode_reward=46.03 +/- 5.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3457     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3457     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115216, episode_reward=47.90 +/- 8.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 115216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016883101 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.41         |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    value_loss           | 8.72         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=115216, episode_reward=47.90 +/- 8.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 47.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 115216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016883101 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.41         |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    value_loss           | 8.72         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116216, episode_reward=41.64 +/- 7.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116216, episode_reward=41.64 +/- 7.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117216, episode_reward=46.53 +/- 14.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117216, episode_reward=46.53 +/- 14.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118216, episode_reward=43.01 +/- 8.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118216, episode_reward=43.01 +/- 8.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3456     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3456     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119216, episode_reward=41.22 +/- 15.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 41.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 119216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001436621 |\n",
      "|    clip_fraction        | 0.0602      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.74        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 8.4         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=119216, episode_reward=41.22 +/- 15.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 41.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 119216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001436621 |\n",
      "|    clip_fraction        | 0.0602      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.74        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 8.4         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120216, episode_reward=44.60 +/- 7.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120216, episode_reward=44.60 +/- 7.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121216, episode_reward=46.23 +/- 10.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121216, episode_reward=46.23 +/- 10.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122216, episode_reward=40.69 +/- 9.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122216, episode_reward=40.69 +/- 9.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3455     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3455     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123216, episode_reward=41.20 +/- 8.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 123216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015722236 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 7.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=123216, episode_reward=41.20 +/- 8.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 123216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015722236 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 7.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124216, episode_reward=42.16 +/- 8.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124216, episode_reward=42.16 +/- 8.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125216, episode_reward=38.17 +/- 11.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125216, episode_reward=38.17 +/- 11.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126216, episode_reward=37.64 +/- 12.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126216, episode_reward=37.64 +/- 12.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3454     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3454     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127216, episode_reward=44.52 +/- 8.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 127216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013067941 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.06         |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0053      |\n",
      "|    value_loss           | 9.83         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=127216, episode_reward=44.52 +/- 8.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 127216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013067941 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.06         |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0053      |\n",
      "|    value_loss           | 9.83         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128216, episode_reward=38.83 +/- 7.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128216, episode_reward=38.83 +/- 7.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129216, episode_reward=46.82 +/- 10.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129216, episode_reward=46.82 +/- 10.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130216, episode_reward=34.50 +/- 7.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130216, episode_reward=34.50 +/- 7.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3453     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3453     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131216, episode_reward=34.44 +/- 3.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 131216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014088908 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.98         |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0064      |\n",
      "|    value_loss           | 8.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=131216, episode_reward=34.44 +/- 3.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 131216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014088908 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.98         |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0064      |\n",
      "|    value_loss           | 8.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132216, episode_reward=37.07 +/- 5.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132216, episode_reward=37.07 +/- 5.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133216, episode_reward=40.49 +/- 5.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133216, episode_reward=40.49 +/- 5.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134216, episode_reward=34.29 +/- 5.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134216, episode_reward=34.29 +/- 5.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3453     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3453     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135216, episode_reward=34.50 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 135216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017539822 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00562     |\n",
      "|    value_loss           | 7.99         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=135216, episode_reward=34.50 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 135216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017539822 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00562     |\n",
      "|    value_loss           | 7.99         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=136216, episode_reward=32.64 +/- 10.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136216, episode_reward=32.64 +/- 10.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137216, episode_reward=38.46 +/- 10.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137216, episode_reward=38.46 +/- 10.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138216, episode_reward=33.93 +/- 5.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138216, episode_reward=33.93 +/- 5.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139216, episode_reward=31.46 +/- 5.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139216, episode_reward=31.46 +/- 5.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3451     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3451     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140216, episode_reward=41.90 +/- 4.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016420092 |\n",
      "|    clip_fraction        | 0.0658       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.86         |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 8.82         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140216, episode_reward=41.90 +/- 4.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016420092 |\n",
      "|    clip_fraction        | 0.0658       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.86         |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 8.82         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=141216, episode_reward=36.98 +/- 3.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141216, episode_reward=36.98 +/- 3.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142216, episode_reward=36.50 +/- 7.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142216, episode_reward=36.50 +/- 7.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143216, episode_reward=39.08 +/- 7.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143216, episode_reward=39.08 +/- 7.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3451     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3451     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144216, episode_reward=34.24 +/- 9.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012983646 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.83         |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00623     |\n",
      "|    value_loss           | 7.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144216, episode_reward=34.24 +/- 9.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012983646 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.83         |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00623     |\n",
      "|    value_loss           | 7.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=145216, episode_reward=38.98 +/- 4.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145216, episode_reward=38.98 +/- 4.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146216, episode_reward=35.06 +/- 8.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146216, episode_reward=35.06 +/- 8.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147216, episode_reward=35.47 +/- 6.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147216, episode_reward=35.47 +/- 6.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3450     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3450     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148216, episode_reward=38.84 +/- 2.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014864991 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.86         |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    value_loss           | 7.39         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148216, episode_reward=38.84 +/- 2.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014864991 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.86         |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    value_loss           | 7.39         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=149216, episode_reward=37.80 +/- 5.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149216, episode_reward=37.80 +/- 5.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150216, episode_reward=38.56 +/- 4.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150216, episode_reward=38.56 +/- 4.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151216, episode_reward=37.32 +/- 6.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151216, episode_reward=37.32 +/- 6.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3450     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3450     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152216, episode_reward=37.43 +/- 0.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015821557 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    value_loss           | 7.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152216, episode_reward=37.43 +/- 0.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015821557 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    value_loss           | 7.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=153216, episode_reward=36.84 +/- 3.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153216, episode_reward=36.84 +/- 3.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154216, episode_reward=38.65 +/- 5.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154216, episode_reward=38.65 +/- 5.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155216, episode_reward=38.33 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155216, episode_reward=38.33 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3449     |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3449     |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156216, episode_reward=35.66 +/- 6.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012555665 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.19         |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.00547     |\n",
      "|    value_loss           | 7.61         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156216, episode_reward=35.66 +/- 6.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012555665 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.19         |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.00547     |\n",
      "|    value_loss           | 7.61         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=157216, episode_reward=40.97 +/- 2.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157216, episode_reward=40.97 +/- 2.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158216, episode_reward=35.74 +/- 7.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158216, episode_reward=35.74 +/- 7.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159216, episode_reward=39.14 +/- 4.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159216, episode_reward=39.14 +/- 4.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3449     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3449     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160216, episode_reward=36.44 +/- 5.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001318669 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 4.35        |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 7.71        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160216, episode_reward=36.44 +/- 5.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001318669 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 4.35        |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 7.71        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=161216, episode_reward=34.51 +/- 6.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161216, episode_reward=34.51 +/- 6.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162216, episode_reward=31.79 +/- 3.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162216, episode_reward=31.79 +/- 3.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163216, episode_reward=37.41 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163216, episode_reward=37.41 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3448     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3448     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164216, episode_reward=39.06 +/- 3.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016679673 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.36         |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.00646     |\n",
      "|    value_loss           | 7.7          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164216, episode_reward=39.06 +/- 3.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016679673 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.36         |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.00646     |\n",
      "|    value_loss           | 7.7          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=165216, episode_reward=40.34 +/- 3.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165216, episode_reward=40.34 +/- 3.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166216, episode_reward=40.96 +/- 9.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166216, episode_reward=40.96 +/- 9.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167216, episode_reward=38.42 +/- 7.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167216, episode_reward=38.42 +/- 7.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3448     |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3448     |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168216, episode_reward=35.13 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 35.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001459767 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.63        |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    value_loss           | 7.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168216, episode_reward=35.13 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 35.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001459767 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.63        |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    value_loss           | 7.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=169216, episode_reward=34.80 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169216, episode_reward=34.80 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170216, episode_reward=38.88 +/- 4.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170216, episode_reward=38.88 +/- 4.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171216, episode_reward=37.86 +/- 6.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171216, episode_reward=37.86 +/- 6.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3447     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3447     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172216, episode_reward=35.97 +/- 8.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012808171 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.8          |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.00446     |\n",
      "|    value_loss           | 7.6          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=172216, episode_reward=35.97 +/- 8.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 172216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012808171 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.8          |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.00446     |\n",
      "|    value_loss           | 7.6          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=173216, episode_reward=38.53 +/- 4.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173216, episode_reward=38.53 +/- 4.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174216, episode_reward=34.03 +/- 3.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174216, episode_reward=34.03 +/- 3.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175216, episode_reward=39.79 +/- 6.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175216, episode_reward=39.79 +/- 6.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3446     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3446     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176216, episode_reward=38.92 +/- 10.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001489063 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.48        |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    value_loss           | 7.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=176216, episode_reward=38.92 +/- 10.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001489063 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.48        |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    value_loss           | 7.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=177216, episode_reward=41.81 +/- 7.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177216, episode_reward=41.81 +/- 7.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178216, episode_reward=38.76 +/- 9.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178216, episode_reward=38.76 +/- 9.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179216, episode_reward=33.86 +/- 3.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179216, episode_reward=33.86 +/- 3.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180216, episode_reward=36.20 +/- 3.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180216, episode_reward=36.20 +/- 3.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3445     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3445     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181216, episode_reward=39.30 +/- 2.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013116996 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    value_loss           | 7.7          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=181216, episode_reward=39.30 +/- 2.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013116996 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    value_loss           | 7.7          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=182216, episode_reward=42.48 +/- 9.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182216, episode_reward=42.48 +/- 9.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183216, episode_reward=40.58 +/- 8.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183216, episode_reward=40.58 +/- 8.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184216, episode_reward=37.55 +/- 3.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184216, episode_reward=37.55 +/- 3.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185216, episode_reward=45.43 +/- 6.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 45.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 185216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013620099 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.15         |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.00662     |\n",
      "|    value_loss           | 8.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=185216, episode_reward=45.43 +/- 6.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 45.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 185216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013620099 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.15         |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.00662     |\n",
      "|    value_loss           | 8.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=186216, episode_reward=39.87 +/- 4.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186216, episode_reward=39.87 +/- 4.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187216, episode_reward=43.17 +/- 9.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187216, episode_reward=43.17 +/- 9.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188216, episode_reward=41.15 +/- 9.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188216, episode_reward=41.15 +/- 9.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189216, episode_reward=44.86 +/- 7.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 189216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012689937 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.00591     |\n",
      "|    value_loss           | 7.28         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=189216, episode_reward=44.86 +/- 7.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 189216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012689937 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.00591     |\n",
      "|    value_loss           | 7.28         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=190216, episode_reward=38.28 +/- 5.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190216, episode_reward=38.28 +/- 5.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191216, episode_reward=37.36 +/- 9.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191216, episode_reward=37.36 +/- 9.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192216, episode_reward=31.96 +/- 2.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192216, episode_reward=31.96 +/- 2.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193216, episode_reward=38.35 +/- 7.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 193216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013702221 |\n",
      "|    clip_fraction        | 0.063        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.66         |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 7.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=193216, episode_reward=38.35 +/- 7.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 193216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013702221 |\n",
      "|    clip_fraction        | 0.063        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.66         |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 7.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=194216, episode_reward=34.85 +/- 5.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194216, episode_reward=34.85 +/- 5.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195216, episode_reward=36.24 +/- 6.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195216, episode_reward=36.24 +/- 6.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196216, episode_reward=33.61 +/- 4.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196216, episode_reward=33.61 +/- 4.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197216, episode_reward=40.99 +/- 8.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 197216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015779232 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.59         |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    value_loss           | 7            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=197216, episode_reward=40.99 +/- 8.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 197216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015779232 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.59         |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    value_loss           | 7            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=198216, episode_reward=35.43 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198216, episode_reward=35.43 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199216, episode_reward=34.40 +/- 4.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199216, episode_reward=34.40 +/- 4.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200216, episode_reward=37.99 +/- 3.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200216, episode_reward=37.99 +/- 3.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3444     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201216, episode_reward=42.24 +/- 2.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 201216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012876343 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.93         |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.00618     |\n",
      "|    value_loss           | 7.32         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=201216, episode_reward=42.24 +/- 2.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 201216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012876343 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.93         |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.00618     |\n",
      "|    value_loss           | 7.32         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=202216, episode_reward=32.18 +/- 4.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202216, episode_reward=32.18 +/- 4.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203216, episode_reward=40.04 +/- 7.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203216, episode_reward=40.04 +/- 7.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204216, episode_reward=35.70 +/- 6.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204216, episode_reward=35.70 +/- 6.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3443     |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3443     |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205216, episode_reward=37.19 +/- 3.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 205216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016823618 |\n",
      "|    clip_fraction        | 0.08         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.00781     |\n",
      "|    value_loss           | 6.85         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=205216, episode_reward=37.19 +/- 3.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 205216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016823618 |\n",
      "|    clip_fraction        | 0.08         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.00781     |\n",
      "|    value_loss           | 6.85         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=206216, episode_reward=38.56 +/- 2.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206216, episode_reward=38.56 +/- 2.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207216, episode_reward=35.63 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207216, episode_reward=35.63 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208216, episode_reward=34.02 +/- 5.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208216, episode_reward=34.02 +/- 5.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3443     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3443     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209216, episode_reward=39.02 +/- 5.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 209216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016093117 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.7          |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    value_loss           | 7.89         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=209216, episode_reward=39.02 +/- 5.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 209216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016093117 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.7          |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    value_loss           | 7.89         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=210216, episode_reward=34.45 +/- 7.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210216, episode_reward=34.45 +/- 7.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211216, episode_reward=33.02 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211216, episode_reward=33.02 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212216, episode_reward=39.00 +/- 2.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212216, episode_reward=39.00 +/- 2.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3443     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3443     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213216, episode_reward=43.23 +/- 6.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 213216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015485202 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.69         |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 7.87         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=213216, episode_reward=43.23 +/- 6.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 213216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015485202 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.69         |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 7.87         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=214216, episode_reward=40.32 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214216, episode_reward=40.32 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215216, episode_reward=46.43 +/- 4.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215216, episode_reward=46.43 +/- 4.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216216, episode_reward=35.73 +/- 8.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216216, episode_reward=35.73 +/- 8.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3442     |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3442     |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217216, episode_reward=43.18 +/- 5.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 217216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012192165 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.73         |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    value_loss           | 8.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=217216, episode_reward=43.18 +/- 5.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 217216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012192165 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.73         |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    value_loss           | 8.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=218216, episode_reward=43.67 +/- 4.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218216, episode_reward=43.67 +/- 4.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219216, episode_reward=46.61 +/- 4.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219216, episode_reward=46.61 +/- 4.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220216, episode_reward=41.29 +/- 3.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220216, episode_reward=41.29 +/- 3.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3439     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3439     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221216, episode_reward=38.72 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 221216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014277393 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.58         |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.00784     |\n",
      "|    value_loss           | 7.8          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=221216, episode_reward=38.72 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 221216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014277393 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.58         |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.00784     |\n",
      "|    value_loss           | 7.8          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=222216, episode_reward=38.46 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222216, episode_reward=38.46 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223216, episode_reward=39.81 +/- 3.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223216, episode_reward=39.81 +/- 3.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224216, episode_reward=45.04 +/- 6.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224216, episode_reward=45.04 +/- 6.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225216, episode_reward=39.04 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225216, episode_reward=39.04 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226216, episode_reward=42.65 +/- 3.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 226216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014791964 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 7.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=226216, episode_reward=42.65 +/- 3.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 226216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014791964 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 7.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=227216, episode_reward=39.38 +/- 3.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227216, episode_reward=39.38 +/- 3.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228216, episode_reward=38.41 +/- 3.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228216, episode_reward=38.41 +/- 3.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229216, episode_reward=44.11 +/- 5.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229216, episode_reward=44.11 +/- 5.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230216, episode_reward=41.82 +/- 2.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016913537 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.79         |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 7.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=230216, episode_reward=41.82 +/- 2.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016913537 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.79         |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 7.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=231216, episode_reward=37.62 +/- 4.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231216, episode_reward=37.62 +/- 4.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232216, episode_reward=39.69 +/- 4.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232216, episode_reward=39.69 +/- 4.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233216, episode_reward=41.74 +/- 2.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233216, episode_reward=41.74 +/- 2.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234216, episode_reward=44.09 +/- 5.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 234216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016374846 |\n",
      "|    clip_fraction        | 0.0786       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.59         |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.00719     |\n",
      "|    value_loss           | 7.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=234216, episode_reward=44.09 +/- 5.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 234216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016374846 |\n",
      "|    clip_fraction        | 0.0786       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.59         |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.00719     |\n",
      "|    value_loss           | 7.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=235216, episode_reward=40.27 +/- 4.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235216, episode_reward=40.27 +/- 4.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236216, episode_reward=39.82 +/- 7.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236216, episode_reward=39.82 +/- 7.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237216, episode_reward=46.09 +/- 4.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237216, episode_reward=46.09 +/- 4.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238216, episode_reward=39.29 +/- 8.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 238216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015748527 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.35         |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 7.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=238216, episode_reward=39.29 +/- 8.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 238216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015748527 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.35         |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 7.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=239216, episode_reward=39.61 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239216, episode_reward=39.61 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240216, episode_reward=37.45 +/- 4.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240216, episode_reward=37.45 +/- 4.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241216, episode_reward=40.75 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241216, episode_reward=40.75 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242216, episode_reward=40.16 +/- 5.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 242216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011516258 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.6          |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00465     |\n",
      "|    value_loss           | 7.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=242216, episode_reward=40.16 +/- 5.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 242216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011516258 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.6          |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00465     |\n",
      "|    value_loss           | 7.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=243216, episode_reward=37.53 +/- 3.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243216, episode_reward=37.53 +/- 3.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244216, episode_reward=42.88 +/- 5.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244216, episode_reward=42.88 +/- 5.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245216, episode_reward=41.43 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245216, episode_reward=41.43 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246216, episode_reward=46.59 +/- 6.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 46.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 246216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013884646 |\n",
      "|    clip_fraction        | 0.0579       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.73         |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.00618     |\n",
      "|    value_loss           | 7.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=246216, episode_reward=46.59 +/- 6.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 46.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 246216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013884646 |\n",
      "|    clip_fraction        | 0.0579       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.73         |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.00618     |\n",
      "|    value_loss           | 7.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=247216, episode_reward=41.77 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247216, episode_reward=41.77 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248216, episode_reward=43.68 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248216, episode_reward=43.68 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249216, episode_reward=41.12 +/- 2.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249216, episode_reward=41.12 +/- 2.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 249856   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 249856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250216, episode_reward=40.48 +/- 6.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 250216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013467706 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.06         |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 7.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=250216, episode_reward=40.48 +/- 6.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 250216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013467706 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.06         |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 7.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=251216, episode_reward=36.16 +/- 5.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251216, episode_reward=36.16 +/- 5.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252216, episode_reward=36.43 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252216, episode_reward=36.43 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253216, episode_reward=39.38 +/- 7.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253216, episode_reward=39.38 +/- 7.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3436     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254216, episode_reward=40.44 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 254216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015124722 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.8          |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=254216, episode_reward=40.44 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 254216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015124722 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.8          |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=255216, episode_reward=41.23 +/- 4.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255216, episode_reward=41.23 +/- 4.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256216, episode_reward=45.05 +/- 6.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256216, episode_reward=45.05 +/- 6.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257216, episode_reward=45.67 +/- 6.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257216, episode_reward=45.67 +/- 6.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258216, episode_reward=41.42 +/- 6.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 258216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014157586 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.29         |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    value_loss           | 7.19         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=258216, episode_reward=41.42 +/- 6.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 258216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014157586 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.29         |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    value_loss           | 7.19         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=259216, episode_reward=41.48 +/- 3.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259216, episode_reward=41.48 +/- 3.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260216, episode_reward=45.41 +/- 2.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260216, episode_reward=45.41 +/- 2.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261216, episode_reward=40.17 +/- 3.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261216, episode_reward=40.17 +/- 3.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262216, episode_reward=46.08 +/- 7.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 46.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001468883 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.73        |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0071     |\n",
      "|    value_loss           | 7.54        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=262216, episode_reward=46.08 +/- 7.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 46.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001468883 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.73        |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0071     |\n",
      "|    value_loss           | 7.54        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=263216, episode_reward=42.95 +/- 3.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263216, episode_reward=42.95 +/- 3.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264216, episode_reward=40.48 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264216, episode_reward=40.48 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265216, episode_reward=39.01 +/- 6.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265216, episode_reward=39.01 +/- 6.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266216, episode_reward=46.68 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266216, episode_reward=46.68 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 266240   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 266240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267216, episode_reward=39.74 +/- 10.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 267216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012426688 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.84         |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    value_loss           | 7.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=267216, episode_reward=39.74 +/- 10.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 267216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012426688 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.84         |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    value_loss           | 7.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=268216, episode_reward=45.00 +/- 8.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268216, episode_reward=45.00 +/- 8.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269216, episode_reward=42.84 +/- 5.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269216, episode_reward=42.84 +/- 5.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270216, episode_reward=39.53 +/- 7.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270216, episode_reward=39.53 +/- 7.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271216, episode_reward=39.24 +/- 7.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 271216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013604241 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.23         |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    value_loss           | 7.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=271216, episode_reward=39.24 +/- 7.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 271216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013604241 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.23         |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    value_loss           | 7.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=272216, episode_reward=43.16 +/- 7.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272216, episode_reward=43.16 +/- 7.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273216, episode_reward=40.47 +/- 6.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273216, episode_reward=40.47 +/- 6.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274216, episode_reward=38.52 +/- 7.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274216, episode_reward=38.52 +/- 7.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 274432   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 274432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275216, episode_reward=44.73 +/- 1.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 275216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015918913 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.79         |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 6.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=275216, episode_reward=44.73 +/- 1.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 275216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015918913 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.79         |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 6.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=276216, episode_reward=42.49 +/- 6.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276216, episode_reward=42.49 +/- 6.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277216, episode_reward=40.10 +/- 6.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277216, episode_reward=40.10 +/- 6.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278216, episode_reward=44.11 +/- 7.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278216, episode_reward=44.11 +/- 7.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279216, episode_reward=46.54 +/- 4.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 46.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 279216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015056352 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.37         |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 7.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=279216, episode_reward=46.54 +/- 4.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 46.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 279216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015056352 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.37         |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 7.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280216, episode_reward=43.73 +/- 7.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280216, episode_reward=43.73 +/- 7.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281216, episode_reward=37.51 +/- 9.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281216, episode_reward=37.51 +/- 9.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282216, episode_reward=44.84 +/- 2.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282216, episode_reward=44.84 +/- 2.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283216, episode_reward=41.54 +/- 3.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 283216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015216144 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.31         |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.00518     |\n",
      "|    value_loss           | 7.64         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=283216, episode_reward=41.54 +/- 3.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 283216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015216144 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.31         |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.00518     |\n",
      "|    value_loss           | 7.64         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=284216, episode_reward=44.92 +/- 5.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284216, episode_reward=44.92 +/- 5.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285216, episode_reward=45.20 +/- 7.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285216, episode_reward=45.20 +/- 7.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286216, episode_reward=45.62 +/- 8.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286216, episode_reward=45.62 +/- 8.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287216, episode_reward=41.59 +/- 5.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 287216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016820712 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.33         |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.00787     |\n",
      "|    value_loss           | 7.79         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=287216, episode_reward=41.59 +/- 5.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 287216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016820712 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.33         |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.00787     |\n",
      "|    value_loss           | 7.79         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288216, episode_reward=39.50 +/- 8.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288216, episode_reward=39.50 +/- 8.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289216, episode_reward=41.39 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289216, episode_reward=41.39 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290216, episode_reward=39.95 +/- 6.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290216, episode_reward=39.95 +/- 6.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291216, episode_reward=46.50 +/- 4.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 46.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 291216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001566402 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.38        |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.00596    |\n",
      "|    value_loss           | 7.38        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=291216, episode_reward=46.50 +/- 4.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 46.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 291216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001566402 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.38        |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.00596    |\n",
      "|    value_loss           | 7.38        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=292216, episode_reward=41.48 +/- 10.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292216, episode_reward=41.48 +/- 10.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293216, episode_reward=40.79 +/- 1.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293216, episode_reward=40.79 +/- 1.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294216, episode_reward=37.52 +/- 5.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294216, episode_reward=37.52 +/- 5.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295216, episode_reward=40.26 +/- 4.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 295216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018591252 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.1          |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.00498     |\n",
      "|    value_loss           | 7.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=295216, episode_reward=40.26 +/- 4.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 295216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018591252 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.1          |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.00498     |\n",
      "|    value_loss           | 7.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=296216, episode_reward=43.85 +/- 3.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296216, episode_reward=43.85 +/- 3.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297216, episode_reward=41.80 +/- 5.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297216, episode_reward=41.80 +/- 5.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298216, episode_reward=44.49 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298216, episode_reward=44.49 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299216, episode_reward=44.19 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 299216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017618263 |\n",
      "|    clip_fraction        | 0.0777       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.05         |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00814     |\n",
      "|    value_loss           | 7.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=299216, episode_reward=44.19 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 299216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017618263 |\n",
      "|    clip_fraction        | 0.0777       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.05         |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00814     |\n",
      "|    value_loss           | 7.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300216, episode_reward=44.00 +/- 5.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300216, episode_reward=44.00 +/- 5.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301216, episode_reward=46.95 +/- 4.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301216, episode_reward=46.95 +/- 4.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302216, episode_reward=43.76 +/- 5.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302216, episode_reward=43.76 +/- 5.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303216, episode_reward=43.22 +/- 2.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 303216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016036013 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.32         |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.00808     |\n",
      "|    value_loss           | 7.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=303216, episode_reward=43.22 +/- 2.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 303216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016036013 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.32         |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.00808     |\n",
      "|    value_loss           | 7.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=304216, episode_reward=45.31 +/- 5.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304216, episode_reward=45.31 +/- 5.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305216, episode_reward=48.08 +/- 5.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305216, episode_reward=48.08 +/- 5.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306216, episode_reward=44.67 +/- 3.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306216, episode_reward=44.67 +/- 3.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307216, episode_reward=42.83 +/- 2.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 307216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014802585 |\n",
      "|    clip_fraction        | 0.0645       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.00663     |\n",
      "|    value_loss           | 6.97         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=307216, episode_reward=42.83 +/- 2.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 307216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014802585 |\n",
      "|    clip_fraction        | 0.0645       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.00663     |\n",
      "|    value_loss           | 6.97         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308216, episode_reward=43.31 +/- 3.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308216, episode_reward=43.31 +/- 3.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309216, episode_reward=44.30 +/- 6.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309216, episode_reward=44.30 +/- 6.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310216, episode_reward=45.41 +/- 3.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310216, episode_reward=45.41 +/- 3.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311216, episode_reward=42.81 +/- 3.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311216, episode_reward=42.81 +/- 3.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312216, episode_reward=44.42 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 312216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012098181 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.52         |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.00619     |\n",
      "|    value_loss           | 6.29         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=312216, episode_reward=44.42 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 312216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012098181 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.52         |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.00619     |\n",
      "|    value_loss           | 6.29         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=313216, episode_reward=44.80 +/- 5.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313216, episode_reward=44.80 +/- 5.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314216, episode_reward=41.02 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314216, episode_reward=41.02 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315216, episode_reward=43.68 +/- 5.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315216, episode_reward=43.68 +/- 5.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 315392   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 315392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316216, episode_reward=37.12 +/- 6.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014470618 |\n",
      "|    clip_fraction        | 0.0575       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.02         |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.00541     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316216, episode_reward=37.12 +/- 6.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 316216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014470618 |\n",
      "|    clip_fraction        | 0.0575       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.02         |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.00541     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=317216, episode_reward=38.45 +/- 9.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317216, episode_reward=38.45 +/- 9.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318216, episode_reward=36.93 +/- 5.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318216, episode_reward=36.93 +/- 5.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319216, episode_reward=41.46 +/- 8.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319216, episode_reward=41.46 +/- 8.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320216, episode_reward=37.75 +/- 1.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011843415 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.37         |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 7.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320216, episode_reward=37.75 +/- 1.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011843415 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.37         |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 7.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=321216, episode_reward=39.64 +/- 2.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321216, episode_reward=39.64 +/- 2.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322216, episode_reward=36.52 +/- 3.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322216, episode_reward=36.52 +/- 3.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323216, episode_reward=36.86 +/- 2.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323216, episode_reward=36.86 +/- 2.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 323584   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 323584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324216, episode_reward=38.72 +/- 2.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012324003 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.06         |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 7.42         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=324216, episode_reward=38.72 +/- 2.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012324003 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.06         |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 7.42         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=325216, episode_reward=40.23 +/- 5.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325216, episode_reward=40.23 +/- 5.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326216, episode_reward=37.61 +/- 3.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326216, episode_reward=37.61 +/- 3.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327216, episode_reward=38.84 +/- 4.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327216, episode_reward=38.84 +/- 4.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328216, episode_reward=35.39 +/- 4.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011580013 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.15         |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    value_loss           | 7.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=328216, episode_reward=35.39 +/- 4.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011580013 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 5.15         |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    value_loss           | 7.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=329216, episode_reward=36.22 +/- 6.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329216, episode_reward=36.22 +/- 6.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330216, episode_reward=38.31 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330216, episode_reward=38.31 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331216, episode_reward=36.65 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331216, episode_reward=36.65 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332216, episode_reward=37.24 +/- 9.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015511082 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.61         |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    value_loss           | 6.7          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=332216, episode_reward=37.24 +/- 9.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015511082 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.61         |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    value_loss           | 6.7          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=333216, episode_reward=41.34 +/- 8.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333216, episode_reward=41.34 +/- 8.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334216, episode_reward=37.20 +/- 2.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334216, episode_reward=37.20 +/- 2.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335216, episode_reward=37.23 +/- 4.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335216, episode_reward=37.23 +/- 4.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 335872   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 335872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336216, episode_reward=41.23 +/- 7.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016492793 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.89         |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.00543     |\n",
      "|    value_loss           | 6.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336216, episode_reward=41.23 +/- 7.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016492793 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.89         |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.00543     |\n",
      "|    value_loss           | 6.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=337216, episode_reward=35.72 +/- 3.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337216, episode_reward=35.72 +/- 3.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338216, episode_reward=34.90 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338216, episode_reward=34.90 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339216, episode_reward=37.19 +/- 6.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339216, episode_reward=37.19 +/- 6.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 339968   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 339968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340216, episode_reward=39.65 +/- 3.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017001089 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.07         |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0065      |\n",
      "|    value_loss           | 6.65         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340216, episode_reward=39.65 +/- 3.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017001089 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.07         |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0065      |\n",
      "|    value_loss           | 6.65         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=341216, episode_reward=44.81 +/- 3.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341216, episode_reward=44.81 +/- 3.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342216, episode_reward=42.03 +/- 1.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342216, episode_reward=42.03 +/- 1.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343216, episode_reward=39.30 +/- 6.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343216, episode_reward=39.30 +/- 6.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344216, episode_reward=45.39 +/- 1.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 45.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 344216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013556961 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.65         |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 7.27         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=344216, episode_reward=45.39 +/- 1.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 45.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 344216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013556961 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.65         |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 7.27         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=345216, episode_reward=42.70 +/- 5.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345216, episode_reward=42.70 +/- 5.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346216, episode_reward=43.04 +/- 4.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346216, episode_reward=43.04 +/- 4.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347216, episode_reward=44.88 +/- 7.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347216, episode_reward=44.88 +/- 7.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 348160   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 348160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348216, episode_reward=44.35 +/- 6.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012264149 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 5.61         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=348216, episode_reward=44.35 +/- 6.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012264149 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 5.61         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=349216, episode_reward=43.02 +/- 1.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349216, episode_reward=43.02 +/- 1.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350216, episode_reward=40.75 +/- 5.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350216, episode_reward=40.75 +/- 5.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351216, episode_reward=44.25 +/- 3.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351216, episode_reward=44.25 +/- 3.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352216, episode_reward=45.74 +/- 6.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352216, episode_reward=45.74 +/- 6.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353216, episode_reward=42.95 +/- 1.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 353216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015819218 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.84         |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0066      |\n",
      "|    value_loss           | 7.43         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=353216, episode_reward=42.95 +/- 1.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 353216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015819218 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.84         |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0066      |\n",
      "|    value_loss           | 7.43         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=354216, episode_reward=42.61 +/- 2.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354216, episode_reward=42.61 +/- 2.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355216, episode_reward=43.54 +/- 3.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355216, episode_reward=43.54 +/- 3.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356216, episode_reward=41.69 +/- 5.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356216, episode_reward=41.69 +/- 5.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 356352   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 356352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357216, episode_reward=40.02 +/- 2.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 357216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014524024 |\n",
      "|    clip_fraction        | 0.0719       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 6.88         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=357216, episode_reward=40.02 +/- 2.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 357216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014524024 |\n",
      "|    clip_fraction        | 0.0719       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 6.88         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=358216, episode_reward=39.25 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358216, episode_reward=39.25 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359216, episode_reward=43.03 +/- 2.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359216, episode_reward=43.03 +/- 2.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360216, episode_reward=43.60 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360216, episode_reward=43.60 +/- 6.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361216, episode_reward=41.46 +/- 1.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 361216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017024954 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00668     |\n",
      "|    value_loss           | 6.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=361216, episode_reward=41.46 +/- 1.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 361216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017024954 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00668     |\n",
      "|    value_loss           | 6.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=362216, episode_reward=43.59 +/- 2.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362216, episode_reward=43.59 +/- 2.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363216, episode_reward=40.92 +/- 9.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363216, episode_reward=40.92 +/- 9.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364216, episode_reward=39.58 +/- 3.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364216, episode_reward=39.58 +/- 3.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 364544   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 364544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365216, episode_reward=39.48 +/- 5.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 365216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014200828 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.87         |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.00633     |\n",
      "|    value_loss           | 6.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=365216, episode_reward=39.48 +/- 5.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 365216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014200828 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.87         |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.00633     |\n",
      "|    value_loss           | 6.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=366216, episode_reward=44.46 +/- 2.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366216, episode_reward=44.46 +/- 2.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367216, episode_reward=41.83 +/- 5.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367216, episode_reward=41.83 +/- 5.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368216, episode_reward=37.93 +/- 7.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368216, episode_reward=37.93 +/- 7.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 368640   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 368640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369216, episode_reward=42.91 +/- 0.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 369216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012776545 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 7.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=369216, episode_reward=42.91 +/- 0.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 369216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012776545 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 7.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=370216, episode_reward=38.45 +/- 4.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370216, episode_reward=38.45 +/- 4.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371216, episode_reward=40.84 +/- 7.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371216, episode_reward=40.84 +/- 7.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372216, episode_reward=39.64 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372216, episode_reward=39.64 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 372736   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 372736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=373216, episode_reward=39.86 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 373216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018062929 |\n",
      "|    clip_fraction        | 0.0836       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.6          |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.00783     |\n",
      "|    value_loss           | 6.44         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=373216, episode_reward=39.86 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 373216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018062929 |\n",
      "|    clip_fraction        | 0.0836       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.6          |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.00783     |\n",
      "|    value_loss           | 6.44         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=374216, episode_reward=40.53 +/- 5.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374216, episode_reward=40.53 +/- 5.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375216, episode_reward=40.82 +/- 2.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375216, episode_reward=40.82 +/- 2.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376216, episode_reward=40.83 +/- 3.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376216, episode_reward=40.83 +/- 3.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377216, episode_reward=40.11 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 377216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012572176 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 6.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=377216, episode_reward=40.11 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 377216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012572176 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 6.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=378216, episode_reward=41.42 +/- 2.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378216, episode_reward=41.42 +/- 2.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379216, episode_reward=42.91 +/- 2.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379216, episode_reward=42.91 +/- 2.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380216, episode_reward=41.03 +/- 5.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380216, episode_reward=41.03 +/- 5.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381216, episode_reward=42.04 +/- 4.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 381216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012184614 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.01         |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 7.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=381216, episode_reward=42.04 +/- 4.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 381216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012184614 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.01         |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 7.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=382216, episode_reward=41.23 +/- 9.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382216, episode_reward=41.23 +/- 9.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383216, episode_reward=39.04 +/- 5.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383216, episode_reward=39.04 +/- 5.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384216, episode_reward=42.64 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384216, episode_reward=42.64 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385216, episode_reward=37.31 +/- 6.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 385216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015607531 |\n",
      "|    clip_fraction        | 0.0676       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.00724     |\n",
      "|    value_loss           | 6.76         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=385216, episode_reward=37.31 +/- 6.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 385216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015607531 |\n",
      "|    clip_fraction        | 0.0676       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.00724     |\n",
      "|    value_loss           | 6.76         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=386216, episode_reward=39.48 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386216, episode_reward=39.48 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387216, episode_reward=41.69 +/- 6.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387216, episode_reward=41.69 +/- 6.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388216, episode_reward=39.99 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388216, episode_reward=39.99 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 389120   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 389120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389216, episode_reward=39.96 +/- 5.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 40          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 389216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001746905 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 6.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=389216, episode_reward=39.96 +/- 5.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 40          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 389216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001746905 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 6.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=390216, episode_reward=37.07 +/- 4.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390216, episode_reward=37.07 +/- 4.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391216, episode_reward=35.85 +/- 4.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391216, episode_reward=35.85 +/- 4.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392216, episode_reward=40.40 +/- 3.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392216, episode_reward=40.40 +/- 3.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393216, episode_reward=40.24 +/- 4.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393216, episode_reward=40.24 +/- 4.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3434     |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394216, episode_reward=37.70 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 394216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014630964 |\n",
      "|    clip_fraction        | 0.0677       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.00614     |\n",
      "|    value_loss           | 6.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=394216, episode_reward=37.70 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 394216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014630964 |\n",
      "|    clip_fraction        | 0.0677       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.00614     |\n",
      "|    value_loss           | 6.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=395216, episode_reward=36.74 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395216, episode_reward=36.74 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396216, episode_reward=37.37 +/- 4.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396216, episode_reward=37.37 +/- 4.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397216, episode_reward=36.78 +/- 1.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397216, episode_reward=36.78 +/- 1.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 397312   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3435     |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 397312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398216, episode_reward=37.98 +/- 3.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 398216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016410161 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 5.89         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=398216, episode_reward=37.98 +/- 3.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 398216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016410161 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 5.89         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=399216, episode_reward=35.84 +/- 3.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399216, episode_reward=35.84 +/- 3.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400216, episode_reward=34.93 +/- 5.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400216, episode_reward=34.93 +/- 5.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401216, episode_reward=38.47 +/- 1.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401216, episode_reward=38.47 +/- 1.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402216, episode_reward=32.08 +/- 5.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 402216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012738879 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    value_loss           | 6.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=402216, episode_reward=32.08 +/- 5.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 402216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012738879 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    value_loss           | 6.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=403216, episode_reward=36.72 +/- 3.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403216, episode_reward=36.72 +/- 3.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404216, episode_reward=32.74 +/- 3.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404216, episode_reward=32.74 +/- 3.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405216, episode_reward=37.67 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405216, episode_reward=37.67 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406216, episode_reward=34.72 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 406216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015029043 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.33         |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 8.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=406216, episode_reward=34.72 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 406216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015029043 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.33         |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 8.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=407216, episode_reward=37.20 +/- 1.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407216, episode_reward=37.20 +/- 1.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408216, episode_reward=35.60 +/- 2.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408216, episode_reward=35.60 +/- 2.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409216, episode_reward=37.30 +/- 2.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409216, episode_reward=37.30 +/- 2.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410216, episode_reward=36.55 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 410216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015417221 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    value_loss           | 6.53         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=410216, episode_reward=36.55 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 410216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015417221 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    value_loss           | 6.53         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=411216, episode_reward=34.49 +/- 7.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411216, episode_reward=34.49 +/- 7.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412216, episode_reward=33.79 +/- 5.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412216, episode_reward=33.79 +/- 5.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413216, episode_reward=34.55 +/- 2.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413216, episode_reward=34.55 +/- 2.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 413696   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 413696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414216, episode_reward=39.04 +/- 3.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 414216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014212409 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 6.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=414216, episode_reward=39.04 +/- 3.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 414216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014212409 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 6.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=415216, episode_reward=37.37 +/- 4.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415216, episode_reward=37.37 +/- 4.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416216, episode_reward=38.01 +/- 2.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416216, episode_reward=38.01 +/- 2.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417216, episode_reward=37.57 +/- 5.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417216, episode_reward=37.57 +/- 5.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418216, episode_reward=37.93 +/- 5.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 418216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016795489 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.2          |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 5.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=418216, episode_reward=37.93 +/- 5.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 418216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016795489 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.2          |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 5.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=419216, episode_reward=35.25 +/- 1.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419216, episode_reward=35.25 +/- 1.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420216, episode_reward=36.10 +/- 3.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420216, episode_reward=36.10 +/- 3.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421216, episode_reward=36.07 +/- 2.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421216, episode_reward=36.07 +/- 2.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422216, episode_reward=40.16 +/- 4.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 422216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012279096 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.00587     |\n",
      "|    value_loss           | 6.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=422216, episode_reward=40.16 +/- 4.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 422216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012279096 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.00587     |\n",
      "|    value_loss           | 6.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=423216, episode_reward=40.16 +/- 3.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423216, episode_reward=40.16 +/- 3.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424216, episode_reward=37.80 +/- 7.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424216, episode_reward=37.80 +/- 7.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425216, episode_reward=39.29 +/- 4.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425216, episode_reward=39.29 +/- 4.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3433     |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426216, episode_reward=38.89 +/- 2.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 426216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015693123 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.01         |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.00716     |\n",
      "|    value_loss           | 6.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=426216, episode_reward=38.89 +/- 2.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 426216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015693123 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.01         |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.00716     |\n",
      "|    value_loss           | 6.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=427216, episode_reward=37.28 +/- 5.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427216, episode_reward=37.28 +/- 5.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428216, episode_reward=41.97 +/- 3.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428216, episode_reward=41.97 +/- 3.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429216, episode_reward=39.10 +/- 2.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429216, episode_reward=39.10 +/- 2.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3431     |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3431     |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430216, episode_reward=36.77 +/- 2.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 430216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014770227 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.88         |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.00603     |\n",
      "|    value_loss           | 6.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=430216, episode_reward=36.77 +/- 2.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 430216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014770227 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.88         |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.00603     |\n",
      "|    value_loss           | 6.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=431216, episode_reward=37.50 +/- 1.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431216, episode_reward=37.50 +/- 1.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432216, episode_reward=37.70 +/- 7.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432216, episode_reward=37.70 +/- 7.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433216, episode_reward=38.38 +/- 3.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433216, episode_reward=38.38 +/- 3.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3431     |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 434176   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3431     |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 434176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434216, episode_reward=34.87 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 434216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015065782 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.56         |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.00682     |\n",
      "|    value_loss           | 6.28         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=434216, episode_reward=34.87 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 434216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015065782 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.56         |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.00682     |\n",
      "|    value_loss           | 6.28         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=435216, episode_reward=31.70 +/- 4.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435216, episode_reward=31.70 +/- 4.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436216, episode_reward=32.67 +/- 4.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436216, episode_reward=32.67 +/- 4.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437216, episode_reward=32.06 +/- 2.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437216, episode_reward=32.06 +/- 2.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438216, episode_reward=32.09 +/- 3.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438216, episode_reward=32.09 +/- 3.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 438272   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 438272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439216, episode_reward=37.23 +/- 2.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 439216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015152683 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    value_loss           | 7.03         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=439216, episode_reward=37.23 +/- 2.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 439216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015152683 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    value_loss           | 7.03         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440216, episode_reward=37.69 +/- 3.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440216, episode_reward=37.69 +/- 3.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441216, episode_reward=36.88 +/- 8.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441216, episode_reward=36.88 +/- 8.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442216, episode_reward=38.79 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442216, episode_reward=38.79 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443216, episode_reward=37.90 +/- 2.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013624341 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.17         |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.00632     |\n",
      "|    value_loss           | 7.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=443216, episode_reward=37.90 +/- 2.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013624341 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.17         |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.00632     |\n",
      "|    value_loss           | 7.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=444216, episode_reward=36.96 +/- 2.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444216, episode_reward=36.96 +/- 2.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445216, episode_reward=31.64 +/- 5.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445216, episode_reward=31.64 +/- 5.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446216, episode_reward=36.00 +/- 5.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446216, episode_reward=36.00 +/- 5.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 446464   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 446464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447216, episode_reward=33.87 +/- 3.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 447216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014467031 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.18         |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    value_loss           | 6.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=447216, episode_reward=33.87 +/- 3.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 447216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014467031 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.18         |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    value_loss           | 6.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=448216, episode_reward=33.38 +/- 4.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448216, episode_reward=33.38 +/- 4.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449216, episode_reward=30.86 +/- 3.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449216, episode_reward=30.86 +/- 3.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450216, episode_reward=30.19 +/- 5.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450216, episode_reward=30.19 +/- 5.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3430     |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451216, episode_reward=38.80 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 451216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014782851 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 6.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=451216, episode_reward=38.80 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 451216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014782851 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 6.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=452216, episode_reward=33.61 +/- 4.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452216, episode_reward=33.61 +/- 4.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453216, episode_reward=31.47 +/- 6.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453216, episode_reward=31.47 +/- 6.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454216, episode_reward=34.80 +/- 1.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454216, episode_reward=34.80 +/- 1.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455216, episode_reward=34.34 +/- 2.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 455216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013028464 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.81         |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.00518     |\n",
      "|    value_loss           | 6.82         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=455216, episode_reward=34.34 +/- 2.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 455216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013028464 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.81         |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.00518     |\n",
      "|    value_loss           | 6.82         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=456216, episode_reward=36.62 +/- 2.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456216, episode_reward=36.62 +/- 2.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457216, episode_reward=35.06 +/- 6.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457216, episode_reward=35.06 +/- 6.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458216, episode_reward=33.46 +/- 5.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458216, episode_reward=33.46 +/- 5.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459216, episode_reward=36.51 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 459216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001527686 |\n",
      "|    clip_fraction        | 0.0558      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 5.75        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=459216, episode_reward=36.51 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 459216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001527686 |\n",
      "|    clip_fraction        | 0.0558      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 5.75        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460216, episode_reward=34.88 +/- 2.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460216, episode_reward=34.88 +/- 2.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461216, episode_reward=38.92 +/- 1.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461216, episode_reward=38.92 +/- 1.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462216, episode_reward=36.10 +/- 2.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462216, episode_reward=36.10 +/- 2.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 462848   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 462848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463216, episode_reward=35.61 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013170893 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.00477     |\n",
      "|    value_loss           | 6.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=463216, episode_reward=35.61 +/- 2.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013170893 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.00477     |\n",
      "|    value_loss           | 6.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=464216, episode_reward=36.45 +/- 1.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464216, episode_reward=36.45 +/- 1.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465216, episode_reward=36.24 +/- 1.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465216, episode_reward=36.24 +/- 1.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466216, episode_reward=34.81 +/- 1.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466216, episode_reward=34.81 +/- 1.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467216, episode_reward=37.04 +/- 1.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 37          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 467216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001345242 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.52        |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 5.82        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=467216, episode_reward=37.04 +/- 1.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 37          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 467216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001345242 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.52        |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 5.82        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=468216, episode_reward=34.08 +/- 1.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468216, episode_reward=34.08 +/- 1.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469216, episode_reward=35.90 +/- 2.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 469216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469216, episode_reward=35.90 +/- 2.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 469216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470216, episode_reward=37.96 +/- 4.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470216, episode_reward=37.96 +/- 4.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471216, episode_reward=37.50 +/- 3.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 471216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017241745 |\n",
      "|    clip_fraction        | 0.0744       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2            |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    value_loss           | 5.76         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=471216, episode_reward=37.50 +/- 3.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 471216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017241745 |\n",
      "|    clip_fraction        | 0.0744       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2            |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    value_loss           | 5.76         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=472216, episode_reward=33.76 +/- 2.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472216, episode_reward=33.76 +/- 2.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473216, episode_reward=32.78 +/- 3.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473216, episode_reward=32.78 +/- 3.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474216, episode_reward=35.96 +/- 5.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474216, episode_reward=35.96 +/- 5.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475216, episode_reward=34.24 +/- 4.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 475216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015010296 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.88         |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=475216, episode_reward=34.24 +/- 4.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 475216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015010296 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.88         |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=476216, episode_reward=32.75 +/- 4.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476216, episode_reward=32.75 +/- 4.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477216, episode_reward=35.18 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477216, episode_reward=35.18 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478216, episode_reward=30.18 +/- 2.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478216, episode_reward=30.18 +/- 2.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479216, episode_reward=34.44 +/- 2.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479216, episode_reward=34.44 +/- 2.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 479232   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 479232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480216, episode_reward=33.85 +/- 1.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017110377 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.00603     |\n",
      "|    value_loss           | 6.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480216, episode_reward=33.85 +/- 1.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017110377 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.00603     |\n",
      "|    value_loss           | 6.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=481216, episode_reward=33.52 +/- 4.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481216, episode_reward=33.52 +/- 4.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482216, episode_reward=32.83 +/- 2.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482216, episode_reward=32.83 +/- 2.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483216, episode_reward=34.08 +/- 2.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483216, episode_reward=34.08 +/- 2.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484216, episode_reward=31.09 +/- 3.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 484216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014961662 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.59        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.51         |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.00638     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=484216, episode_reward=31.09 +/- 3.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 484216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014961662 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.59        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.51         |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.00638     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=485216, episode_reward=34.84 +/- 4.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485216, episode_reward=34.84 +/- 4.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486216, episode_reward=33.43 +/- 3.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486216, episode_reward=33.43 +/- 3.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487216, episode_reward=34.81 +/- 3.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487216, episode_reward=34.81 +/- 3.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 487424   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 487424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488216, episode_reward=35.48 +/- 6.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 488216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017960478 |\n",
      "|    clip_fraction        | 0.0702       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.59        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.21         |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    value_loss           | 6.34         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=488216, episode_reward=35.48 +/- 6.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 488216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017960478 |\n",
      "|    clip_fraction        | 0.0702       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.59        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.21         |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    value_loss           | 6.34         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=489216, episode_reward=34.04 +/- 3.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489216, episode_reward=34.04 +/- 3.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490216, episode_reward=34.16 +/- 4.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490216, episode_reward=34.16 +/- 4.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491216, episode_reward=34.20 +/- 2.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491216, episode_reward=34.20 +/- 2.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492216, episode_reward=35.43 +/- 1.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 35.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 492216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00155621 |\n",
      "|    clip_fraction        | 0.0658     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 2.88       |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.00656   |\n",
      "|    value_loss           | 5.87       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=492216, episode_reward=35.43 +/- 1.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 35.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 492216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00155621 |\n",
      "|    clip_fraction        | 0.0658     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 2.88       |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.00656   |\n",
      "|    value_loss           | 5.87       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=493216, episode_reward=38.09 +/- 4.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493216, episode_reward=38.09 +/- 4.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494216, episode_reward=31.67 +/- 4.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 494216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494216, episode_reward=31.67 +/- 4.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 494216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495216, episode_reward=36.68 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495216, episode_reward=36.68 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 495616   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 495616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496216, episode_reward=38.06 +/- 2.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 496216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014061206 |\n",
      "|    clip_fraction        | 0.0721       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.82         |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -0.00685     |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=496216, episode_reward=38.06 +/- 2.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 496216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014061206 |\n",
      "|    clip_fraction        | 0.0721       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.82         |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -0.00685     |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=497216, episode_reward=35.55 +/- 3.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497216, episode_reward=35.55 +/- 3.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498216, episode_reward=38.88 +/- 1.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 498216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498216, episode_reward=38.88 +/- 1.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 498216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499216, episode_reward=34.65 +/- 1.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499216, episode_reward=34.65 +/- 1.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 499712   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 499712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500216, episode_reward=40.52 +/- 4.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017540031 |\n",
      "|    clip_fraction        | 0.0686       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.04         |\n",
      "|    n_updates            | 3020         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 6.01         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500216, episode_reward=40.52 +/- 4.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017540031 |\n",
      "|    clip_fraction        | 0.0686       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.04         |\n",
      "|    n_updates            | 3020         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 6.01         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=501216, episode_reward=36.81 +/- 6.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501216, episode_reward=36.81 +/- 6.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502216, episode_reward=43.20 +/- 1.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502216, episode_reward=43.20 +/- 1.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503216, episode_reward=40.22 +/- 0.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503216, episode_reward=40.22 +/- 0.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 47.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 47.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504216, episode_reward=36.03 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001436203 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.43        |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0055     |\n",
      "|    value_loss           | 7.48        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=504216, episode_reward=36.03 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001436203 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.43        |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0055     |\n",
      "|    value_loss           | 7.48        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=505216, episode_reward=36.91 +/- 3.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505216, episode_reward=36.91 +/- 3.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506216, episode_reward=33.59 +/- 2.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 506216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506216, episode_reward=33.59 +/- 2.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 506216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507216, episode_reward=37.08 +/- 4.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507216, episode_reward=37.08 +/- 4.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508216, episode_reward=38.08 +/- 2.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 508216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015114995 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.19         |\n",
      "|    n_updates            | 3060         |\n",
      "|    policy_gradient_loss | -0.00645     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=508216, episode_reward=38.08 +/- 2.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 508216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015114995 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.19         |\n",
      "|    n_updates            | 3060         |\n",
      "|    policy_gradient_loss | -0.00645     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=509216, episode_reward=34.54 +/- 5.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509216, episode_reward=34.54 +/- 5.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510216, episode_reward=36.60 +/- 7.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510216, episode_reward=36.60 +/- 7.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511216, episode_reward=34.72 +/- 2.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511216, episode_reward=34.72 +/- 2.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512216, episode_reward=35.16 +/- 5.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016388664 |\n",
      "|    clip_fraction        | 0.0694       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.51         |\n",
      "|    n_updates            | 3080         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 6.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=512216, episode_reward=35.16 +/- 5.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016388664 |\n",
      "|    clip_fraction        | 0.0694       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.51         |\n",
      "|    n_updates            | 3080         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 6.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=513216, episode_reward=34.26 +/- 9.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513216, episode_reward=34.26 +/- 9.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514216, episode_reward=35.95 +/- 8.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514216, episode_reward=35.95 +/- 8.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515216, episode_reward=35.07 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515216, episode_reward=35.07 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516216, episode_reward=35.96 +/- 8.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018832707 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.83         |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | -0.00619     |\n",
      "|    value_loss           | 6.07         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=516216, episode_reward=35.96 +/- 8.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018832707 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.83         |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | -0.00619     |\n",
      "|    value_loss           | 6.07         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=517216, episode_reward=35.91 +/- 7.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517216, episode_reward=35.91 +/- 7.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518216, episode_reward=39.93 +/- 4.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518216, episode_reward=39.93 +/- 4.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519216, episode_reward=34.35 +/- 7.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519216, episode_reward=34.35 +/- 7.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520216, episode_reward=34.99 +/- 4.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014689739 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.95         |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 6.49         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=520216, episode_reward=34.99 +/- 4.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014689739 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.95         |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 6.49         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=521216, episode_reward=39.18 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521216, episode_reward=39.18 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522216, episode_reward=41.58 +/- 4.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522216, episode_reward=41.58 +/- 4.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523216, episode_reward=39.70 +/- 4.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523216, episode_reward=39.70 +/- 4.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524216, episode_reward=37.41 +/- 8.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524216, episode_reward=37.41 +/- 8.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=525216, episode_reward=33.34 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 525216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015846211 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.58         |\n",
      "|    n_updates            | 3140         |\n",
      "|    policy_gradient_loss | -0.00556     |\n",
      "|    value_loss           | 7.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=525216, episode_reward=33.34 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 525216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015846211 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.58         |\n",
      "|    n_updates            | 3140         |\n",
      "|    policy_gradient_loss | -0.00556     |\n",
      "|    value_loss           | 7.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=526216, episode_reward=39.67 +/- 6.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526216, episode_reward=39.67 +/- 6.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527216, episode_reward=35.00 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527216, episode_reward=35.00 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528216, episode_reward=33.99 +/- 6.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528216, episode_reward=33.99 +/- 6.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529216, episode_reward=41.96 +/- 4.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 529216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014270754 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.25         |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 6.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=529216, episode_reward=41.96 +/- 4.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 529216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014270754 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.25         |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 6.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=530216, episode_reward=38.75 +/- 6.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530216, episode_reward=38.75 +/- 6.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=531216, episode_reward=41.69 +/- 5.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=531216, episode_reward=41.69 +/- 5.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532216, episode_reward=35.91 +/- 8.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532216, episode_reward=35.91 +/- 8.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533216, episode_reward=38.90 +/- 6.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 533216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016074751 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | -0.00693     |\n",
      "|    value_loss           | 5.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=533216, episode_reward=38.90 +/- 6.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 533216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016074751 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | -0.00693     |\n",
      "|    value_loss           | 5.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=534216, episode_reward=38.78 +/- 3.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534216, episode_reward=38.78 +/- 3.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535216, episode_reward=37.05 +/- 6.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 535216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535216, episode_reward=37.05 +/- 6.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 535216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536216, episode_reward=41.71 +/- 1.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536216, episode_reward=41.71 +/- 1.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 536576   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 536576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537216, episode_reward=42.55 +/- 2.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 537216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015402259 |\n",
      "|    clip_fraction        | 0.0598       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.92         |\n",
      "|    n_updates            | 3200         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=537216, episode_reward=42.55 +/- 2.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 537216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015402259 |\n",
      "|    clip_fraction        | 0.0598       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.92         |\n",
      "|    n_updates            | 3200         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=538216, episode_reward=44.38 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538216, episode_reward=44.38 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539216, episode_reward=41.29 +/- 5.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539216, episode_reward=41.29 +/- 5.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540216, episode_reward=43.76 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540216, episode_reward=43.76 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541216, episode_reward=35.02 +/- 8.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 541216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014386671 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.06         |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    value_loss           | 6.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=541216, episode_reward=35.02 +/- 8.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 541216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014386671 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.06         |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    value_loss           | 6.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=542216, episode_reward=34.86 +/- 9.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542216, episode_reward=34.86 +/- 9.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543216, episode_reward=40.98 +/- 2.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 543216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543216, episode_reward=40.98 +/- 2.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 543216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544216, episode_reward=43.45 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544216, episode_reward=43.45 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 544768   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 544768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=545216, episode_reward=38.19 +/- 3.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 545216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014432962 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.39         |\n",
      "|    n_updates            | 3240         |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 6.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=545216, episode_reward=38.19 +/- 3.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 545216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014432962 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.39         |\n",
      "|    n_updates            | 3240         |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 6.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=546216, episode_reward=30.54 +/- 10.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546216, episode_reward=30.54 +/- 10.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547216, episode_reward=40.46 +/- 4.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 547216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547216, episode_reward=40.46 +/- 4.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 547216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548216, episode_reward=35.85 +/- 8.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548216, episode_reward=35.85 +/- 8.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 548864   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 548864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549216, episode_reward=34.30 +/- 8.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 549216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017467141 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.96         |\n",
      "|    n_updates            | 3260         |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    value_loss           | 6.27         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=549216, episode_reward=34.30 +/- 8.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 549216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017467141 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.96         |\n",
      "|    n_updates            | 3260         |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    value_loss           | 6.27         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=550216, episode_reward=40.14 +/- 5.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550216, episode_reward=40.14 +/- 5.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551216, episode_reward=42.89 +/- 3.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551216, episode_reward=42.89 +/- 3.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552216, episode_reward=38.78 +/- 6.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552216, episode_reward=38.78 +/- 6.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=553216, episode_reward=39.66 +/- 4.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 553216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017337475 |\n",
      "|    clip_fraction        | 0.068        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.15         |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.00541     |\n",
      "|    value_loss           | 5.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=553216, episode_reward=39.66 +/- 4.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 553216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017337475 |\n",
      "|    clip_fraction        | 0.068        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.15         |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.00541     |\n",
      "|    value_loss           | 5.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=554216, episode_reward=44.04 +/- 5.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554216, episode_reward=44.04 +/- 5.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555216, episode_reward=41.87 +/- 4.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555216, episode_reward=41.87 +/- 4.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556216, episode_reward=42.40 +/- 3.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556216, episode_reward=42.40 +/- 3.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557216, episode_reward=38.95 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 557216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001880313 |\n",
      "|    clip_fraction        | 0.07        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    value_loss           | 7.7         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=557216, episode_reward=38.95 +/- 6.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 557216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001880313 |\n",
      "|    clip_fraction        | 0.07        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    value_loss           | 7.7         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=558216, episode_reward=36.38 +/- 12.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558216, episode_reward=36.38 +/- 12.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559216, episode_reward=43.03 +/- 1.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559216, episode_reward=43.03 +/- 1.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560216, episode_reward=38.43 +/- 5.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560216, episode_reward=38.43 +/- 5.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561216, episode_reward=38.93 +/- 4.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 561216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015911344 |\n",
      "|    clip_fraction        | 0.0647       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 3320         |\n",
      "|    policy_gradient_loss | -0.00636     |\n",
      "|    value_loss           | 5.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=561216, episode_reward=38.93 +/- 4.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 561216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015911344 |\n",
      "|    clip_fraction        | 0.0647       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 3320         |\n",
      "|    policy_gradient_loss | -0.00636     |\n",
      "|    value_loss           | 5.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=562216, episode_reward=40.54 +/- 4.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562216, episode_reward=40.54 +/- 4.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563216, episode_reward=37.27 +/- 3.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563216, episode_reward=37.27 +/- 3.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564216, episode_reward=39.22 +/- 3.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564216, episode_reward=39.22 +/- 3.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565216, episode_reward=38.56 +/- 4.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565216, episode_reward=38.56 +/- 4.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566216, episode_reward=39.02 +/- 1.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 566216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014500637 |\n",
      "|    clip_fraction        | 0.0706       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.00728     |\n",
      "|    value_loss           | 7.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=566216, episode_reward=39.02 +/- 1.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 566216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014500637 |\n",
      "|    clip_fraction        | 0.0706       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.00728     |\n",
      "|    value_loss           | 7.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=567216, episode_reward=39.77 +/- 1.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567216, episode_reward=39.77 +/- 1.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568216, episode_reward=40.68 +/- 4.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568216, episode_reward=40.68 +/- 4.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569216, episode_reward=39.61 +/- 3.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569216, episode_reward=39.61 +/- 3.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 569344   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 569344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570216, episode_reward=35.15 +/- 5.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 570216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017525568 |\n",
      "|    clip_fraction        | 0.0739       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.63         |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.00552     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=570216, episode_reward=35.15 +/- 5.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 570216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017525568 |\n",
      "|    clip_fraction        | 0.0739       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.63         |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.00552     |\n",
      "|    value_loss           | 6.66         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=571216, episode_reward=37.92 +/- 3.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571216, episode_reward=37.92 +/- 3.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572216, episode_reward=45.50 +/- 4.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572216, episode_reward=45.50 +/- 4.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 45.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573216, episode_reward=38.91 +/- 3.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573216, episode_reward=38.91 +/- 3.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574216, episode_reward=43.30 +/- 7.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 574216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014163624 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.86         |\n",
      "|    n_updates            | 3380         |\n",
      "|    policy_gradient_loss | -0.00555     |\n",
      "|    value_loss           | 6.36         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=574216, episode_reward=43.30 +/- 7.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 574216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014163624 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.86         |\n",
      "|    n_updates            | 3380         |\n",
      "|    policy_gradient_loss | -0.00555     |\n",
      "|    value_loss           | 6.36         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=575216, episode_reward=39.40 +/- 8.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575216, episode_reward=39.40 +/- 8.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576216, episode_reward=43.08 +/- 4.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576216, episode_reward=43.08 +/- 4.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577216, episode_reward=38.75 +/- 0.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577216, episode_reward=38.75 +/- 0.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3429     |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578216, episode_reward=40.57 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 578216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015761133 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.37         |\n",
      "|    n_updates            | 3400         |\n",
      "|    policy_gradient_loss | -0.00622     |\n",
      "|    value_loss           | 6.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=578216, episode_reward=40.57 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 578216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015761133 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.37         |\n",
      "|    n_updates            | 3400         |\n",
      "|    policy_gradient_loss | -0.00622     |\n",
      "|    value_loss           | 6.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=579216, episode_reward=40.42 +/- 1.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579216, episode_reward=40.42 +/- 1.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580216, episode_reward=38.73 +/- 4.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580216, episode_reward=38.73 +/- 4.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581216, episode_reward=42.31 +/- 7.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581216, episode_reward=42.31 +/- 7.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582216, episode_reward=41.70 +/- 3.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 41.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 582216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001422077 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.36        |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    value_loss           | 6.79        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=582216, episode_reward=41.70 +/- 3.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 41.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 582216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001422077 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.36        |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    value_loss           | 6.79        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=583216, episode_reward=39.70 +/- 9.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583216, episode_reward=39.70 +/- 9.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584216, episode_reward=36.89 +/- 3.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 584216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584216, episode_reward=36.89 +/- 3.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 584216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585216, episode_reward=43.47 +/- 2.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585216, episode_reward=43.47 +/- 2.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 585728   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 585728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586216, episode_reward=38.39 +/- 2.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 586216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019483487 |\n",
      "|    clip_fraction        | 0.0901       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.45         |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.00714     |\n",
      "|    value_loss           | 5.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=586216, episode_reward=38.39 +/- 2.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 586216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019483487 |\n",
      "|    clip_fraction        | 0.0901       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.45         |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.00714     |\n",
      "|    value_loss           | 5.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=587216, episode_reward=34.86 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587216, episode_reward=34.86 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588216, episode_reward=36.34 +/- 1.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588216, episode_reward=36.34 +/- 1.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589216, episode_reward=35.56 +/- 6.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589216, episode_reward=35.56 +/- 6.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590216, episode_reward=40.48 +/- 4.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 590216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014889948 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 3460         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 7.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=590216, episode_reward=40.48 +/- 4.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 590216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014889948 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 3460         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 7.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=591216, episode_reward=36.23 +/- 6.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591216, episode_reward=36.23 +/- 6.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592216, episode_reward=33.47 +/- 4.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 592216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592216, episode_reward=33.47 +/- 4.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 592216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593216, episode_reward=30.88 +/- 6.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593216, episode_reward=30.88 +/- 6.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 593920   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 593920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594216, episode_reward=34.89 +/- 6.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 594216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014349234 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.00623     |\n",
      "|    value_loss           | 6.35         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=594216, episode_reward=34.89 +/- 6.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 594216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014349234 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.00623     |\n",
      "|    value_loss           | 6.35         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=595216, episode_reward=37.21 +/- 6.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595216, episode_reward=37.21 +/- 6.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596216, episode_reward=40.68 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 596216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596216, episode_reward=40.68 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 596216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597216, episode_reward=39.02 +/- 2.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597216, episode_reward=39.02 +/- 2.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598216, episode_reward=36.82 +/- 7.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 598216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001504235 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    value_loss           | 5.86        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=598216, episode_reward=36.82 +/- 7.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 36.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 598216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001504235 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    value_loss           | 5.86        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=599216, episode_reward=34.40 +/- 6.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599216, episode_reward=34.40 +/- 6.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600216, episode_reward=37.72 +/- 1.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600216, episode_reward=37.72 +/- 1.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601216, episode_reward=38.26 +/- 1.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601216, episode_reward=38.26 +/- 1.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602216, episode_reward=33.71 +/- 6.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 602216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015140843 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 5.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=602216, episode_reward=33.71 +/- 6.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 602216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015140843 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 5.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=603216, episode_reward=36.08 +/- 6.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603216, episode_reward=36.08 +/- 6.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604216, episode_reward=35.33 +/- 6.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604216, episode_reward=35.33 +/- 6.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605216, episode_reward=32.54 +/- 6.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605216, episode_reward=32.54 +/- 6.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606216, episode_reward=35.28 +/- 5.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 606216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015964354 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.31         |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | -0.00696     |\n",
      "|    value_loss           | 6.22         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=606216, episode_reward=35.28 +/- 5.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 606216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015964354 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.31         |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | -0.00696     |\n",
      "|    value_loss           | 6.22         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=607216, episode_reward=34.18 +/- 1.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=607216, episode_reward=34.18 +/- 1.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608216, episode_reward=34.60 +/- 1.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608216, episode_reward=34.60 +/- 1.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609216, episode_reward=35.75 +/- 2.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609216, episode_reward=35.75 +/- 2.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610216, episode_reward=40.35 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610216, episode_reward=40.35 +/- 3.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=611216, episode_reward=31.85 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 611216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012986863 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.74         |\n",
      "|    n_updates            | 3560         |\n",
      "|    policy_gradient_loss | -0.00529     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=611216, episode_reward=31.85 +/- 5.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 611216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012986863 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.74         |\n",
      "|    n_updates            | 3560         |\n",
      "|    policy_gradient_loss | -0.00529     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=612216, episode_reward=34.57 +/- 6.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612216, episode_reward=34.57 +/- 6.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613216, episode_reward=34.64 +/- 1.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613216, episode_reward=34.64 +/- 1.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614216, episode_reward=31.74 +/- 6.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614216, episode_reward=31.74 +/- 6.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615216, episode_reward=36.11 +/- 2.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 615216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014407812 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 3580         |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 6.73         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=615216, episode_reward=36.11 +/- 2.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 615216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014407812 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 3580         |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 6.73         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=616216, episode_reward=37.89 +/- 2.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616216, episode_reward=37.89 +/- 2.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617216, episode_reward=36.75 +/- 2.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617216, episode_reward=36.75 +/- 2.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618216, episode_reward=32.95 +/- 4.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618216, episode_reward=32.95 +/- 4.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 618496   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 618496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=619216, episode_reward=34.33 +/- 3.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 34.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 619216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001658081 |\n",
      "|    clip_fraction        | 0.0641      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.31        |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 6.41        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=619216, episode_reward=34.33 +/- 3.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 34.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 619216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001658081 |\n",
      "|    clip_fraction        | 0.0641      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 3.31        |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 6.41        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620216, episode_reward=33.53 +/- 2.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620216, episode_reward=33.53 +/- 2.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621216, episode_reward=32.61 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 621216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621216, episode_reward=32.61 +/- 2.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 621216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622216, episode_reward=34.77 +/- 3.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622216, episode_reward=34.77 +/- 3.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623216, episode_reward=32.72 +/- 1.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 623216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018896547 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 3620         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=623216, episode_reward=32.72 +/- 1.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 623216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018896547 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 3620         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=624216, episode_reward=33.30 +/- 2.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624216, episode_reward=33.30 +/- 2.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625216, episode_reward=34.52 +/- 1.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625216, episode_reward=34.52 +/- 1.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626216, episode_reward=34.12 +/- 5.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626216, episode_reward=34.12 +/- 5.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 626688   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3428     |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 626688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627216, episode_reward=35.85 +/- 1.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 627216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014923939 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.15         |\n",
      "|    n_updates            | 3640         |\n",
      "|    policy_gradient_loss | -0.0062      |\n",
      "|    value_loss           | 5.92         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=627216, episode_reward=35.85 +/- 1.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 627216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014923939 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.15         |\n",
      "|    n_updates            | 3640         |\n",
      "|    policy_gradient_loss | -0.0062      |\n",
      "|    value_loss           | 5.92         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=628216, episode_reward=38.10 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628216, episode_reward=38.10 +/- 2.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629216, episode_reward=36.31 +/- 1.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 629216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629216, episode_reward=36.31 +/- 1.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 629216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630216, episode_reward=34.65 +/- 2.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630216, episode_reward=34.65 +/- 2.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3427     |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3427     |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=631216, episode_reward=37.58 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 631216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017321175 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.00627     |\n",
      "|    value_loss           | 6.32         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=631216, episode_reward=37.58 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 631216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017321175 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.00627     |\n",
      "|    value_loss           | 6.32         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=632216, episode_reward=36.96 +/- 1.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632216, episode_reward=36.96 +/- 1.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633216, episode_reward=35.54 +/- 4.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 633216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633216, episode_reward=35.54 +/- 4.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 633216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634216, episode_reward=38.21 +/- 2.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634216, episode_reward=38.21 +/- 2.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 634880   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 634880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635216, episode_reward=35.06 +/- 4.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 635216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016311689 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.48         |\n",
      "|    n_updates            | 3680         |\n",
      "|    policy_gradient_loss | -0.00602     |\n",
      "|    value_loss           | 5.27         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=635216, episode_reward=35.06 +/- 4.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 635216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016311689 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.48         |\n",
      "|    n_updates            | 3680         |\n",
      "|    policy_gradient_loss | -0.00602     |\n",
      "|    value_loss           | 5.27         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=636216, episode_reward=34.38 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636216, episode_reward=34.38 +/- 3.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637216, episode_reward=38.05 +/- 2.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637216, episode_reward=38.05 +/- 2.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638216, episode_reward=37.63 +/- 3.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638216, episode_reward=37.63 +/- 3.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639216, episode_reward=33.48 +/- 2.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 639216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016913482 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.1          |\n",
      "|    n_updates            | 3700         |\n",
      "|    policy_gradient_loss | -0.00708     |\n",
      "|    value_loss           | 5.84         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=639216, episode_reward=33.48 +/- 2.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 639216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016913482 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.1          |\n",
      "|    n_updates            | 3700         |\n",
      "|    policy_gradient_loss | -0.00708     |\n",
      "|    value_loss           | 5.84         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640216, episode_reward=34.23 +/- 1.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640216, episode_reward=34.23 +/- 1.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641216, episode_reward=35.69 +/- 2.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641216, episode_reward=35.69 +/- 2.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642216, episode_reward=34.29 +/- 1.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642216, episode_reward=34.29 +/- 1.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 643072   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 643072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643216, episode_reward=32.86 +/- 4.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 643216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019312296 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 3720         |\n",
      "|    policy_gradient_loss | -0.00679     |\n",
      "|    value_loss           | 6.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=643216, episode_reward=32.86 +/- 4.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 643216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019312296 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 3720         |\n",
      "|    policy_gradient_loss | -0.00679     |\n",
      "|    value_loss           | 6.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=644216, episode_reward=32.48 +/- 4.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644216, episode_reward=32.48 +/- 4.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645216, episode_reward=30.95 +/- 1.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645216, episode_reward=30.95 +/- 1.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646216, episode_reward=30.78 +/- 4.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646216, episode_reward=30.78 +/- 4.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647216, episode_reward=27.78 +/- 3.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 647216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020004686 |\n",
      "|    clip_fraction        | 0.0719       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.48         |\n",
      "|    n_updates            | 3740         |\n",
      "|    policy_gradient_loss | -0.0066      |\n",
      "|    value_loss           | 6.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=647216, episode_reward=27.78 +/- 3.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 647216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020004686 |\n",
      "|    clip_fraction        | 0.0719       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.48         |\n",
      "|    n_updates            | 3740         |\n",
      "|    policy_gradient_loss | -0.0066      |\n",
      "|    value_loss           | 6.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=648216, episode_reward=28.49 +/- 4.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648216, episode_reward=28.49 +/- 4.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649216, episode_reward=32.75 +/- 3.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649216, episode_reward=32.75 +/- 3.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650216, episode_reward=34.29 +/- 5.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650216, episode_reward=34.29 +/- 5.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651216, episode_reward=35.94 +/- 5.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651216, episode_reward=35.94 +/- 5.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652216, episode_reward=29.98 +/- 6.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 652216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016097601 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 3760         |\n",
      "|    policy_gradient_loss | -0.00738     |\n",
      "|    value_loss           | 7.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=652216, episode_reward=29.98 +/- 6.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 652216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016097601 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 3760         |\n",
      "|    policy_gradient_loss | -0.00738     |\n",
      "|    value_loss           | 7.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=653216, episode_reward=34.01 +/- 4.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653216, episode_reward=34.01 +/- 4.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654216, episode_reward=25.01 +/- 1.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654216, episode_reward=25.01 +/- 1.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655216, episode_reward=31.40 +/- 3.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655216, episode_reward=31.40 +/- 3.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3426     |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656216, episode_reward=28.70 +/- 4.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019750667 |\n",
      "|    clip_fraction        | 0.079        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    value_loss           | 5.46         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=656216, episode_reward=28.70 +/- 4.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019750667 |\n",
      "|    clip_fraction        | 0.079        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    value_loss           | 5.46         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=657216, episode_reward=31.15 +/- 4.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657216, episode_reward=31.15 +/- 4.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658216, episode_reward=29.85 +/- 4.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658216, episode_reward=29.85 +/- 4.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659216, episode_reward=27.55 +/- 4.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659216, episode_reward=27.55 +/- 4.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 659456   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 659456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660216, episode_reward=29.77 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014737195 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.61         |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00591     |\n",
      "|    value_loss           | 6.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=660216, episode_reward=29.77 +/- 4.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014737195 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.61         |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00591     |\n",
      "|    value_loss           | 6.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=661216, episode_reward=29.93 +/- 2.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661216, episode_reward=29.93 +/- 2.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662216, episode_reward=28.74 +/- 2.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662216, episode_reward=28.74 +/- 2.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663216, episode_reward=30.51 +/- 5.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663216, episode_reward=30.51 +/- 5.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664216, episode_reward=29.57 +/- 4.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 664216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014530278 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.48         |\n",
      "|    n_updates            | 3820         |\n",
      "|    policy_gradient_loss | -0.00597     |\n",
      "|    value_loss           | 6.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=664216, episode_reward=29.57 +/- 4.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 664216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014530278 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.48         |\n",
      "|    n_updates            | 3820         |\n",
      "|    policy_gradient_loss | -0.00597     |\n",
      "|    value_loss           | 6.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=665216, episode_reward=28.27 +/- 4.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665216, episode_reward=28.27 +/- 4.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666216, episode_reward=28.73 +/- 5.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 666216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666216, episode_reward=28.73 +/- 5.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 666216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667216, episode_reward=31.11 +/- 6.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667216, episode_reward=31.11 +/- 6.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 667648   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 667648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668216, episode_reward=29.36 +/- 5.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 668216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015799738 |\n",
      "|    clip_fraction        | 0.0606       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.06         |\n",
      "|    n_updates            | 3840         |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    value_loss           | 5.9          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=668216, episode_reward=29.36 +/- 5.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 668216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015799738 |\n",
      "|    clip_fraction        | 0.0606       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.06         |\n",
      "|    n_updates            | 3840         |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    value_loss           | 5.9          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=669216, episode_reward=31.73 +/- 3.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669216, episode_reward=31.73 +/- 3.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670216, episode_reward=30.50 +/- 1.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670216, episode_reward=30.50 +/- 1.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671216, episode_reward=28.27 +/- 5.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671216, episode_reward=28.27 +/- 5.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672216, episode_reward=33.48 +/- 3.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 33.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 672216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001608686 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 6.87        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=672216, episode_reward=33.48 +/- 3.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 33.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 672216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001608686 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 6.87        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=673216, episode_reward=33.76 +/- 4.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673216, episode_reward=33.76 +/- 4.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674216, episode_reward=34.34 +/- 6.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674216, episode_reward=34.34 +/- 6.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675216, episode_reward=33.59 +/- 3.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675216, episode_reward=33.59 +/- 3.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3425     |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676216, episode_reward=35.68 +/- 4.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 676216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016936439 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.53         |\n",
      "|    n_updates            | 3880         |\n",
      "|    policy_gradient_loss | -0.00646     |\n",
      "|    value_loss           | 5.73         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=676216, episode_reward=35.68 +/- 4.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 676216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016936439 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.53         |\n",
      "|    n_updates            | 3880         |\n",
      "|    policy_gradient_loss | -0.00646     |\n",
      "|    value_loss           | 5.73         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=677216, episode_reward=33.58 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677216, episode_reward=33.58 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678216, episode_reward=31.42 +/- 2.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 678216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678216, episode_reward=31.42 +/- 2.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 678216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679216, episode_reward=34.11 +/- 5.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679216, episode_reward=34.11 +/- 5.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 198      |\n",
      "|    total_timesteps | 679936   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 198      |\n",
      "|    total_timesteps | 679936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680216, episode_reward=30.95 +/- 5.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 680216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017490818 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.69         |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    value_loss           | 5.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=680216, episode_reward=30.95 +/- 5.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 680216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017490818 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.69         |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    value_loss           | 5.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=681216, episode_reward=30.64 +/- 4.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681216, episode_reward=30.64 +/- 4.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682216, episode_reward=27.47 +/- 4.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 682216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682216, episode_reward=27.47 +/- 4.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 682216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683216, episode_reward=31.40 +/- 5.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683216, episode_reward=31.40 +/- 5.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 684032   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 684032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684216, episode_reward=31.81 +/- 4.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 684216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017334924 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 3920         |\n",
      "|    policy_gradient_loss | -0.00657     |\n",
      "|    value_loss           | 5.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=684216, episode_reward=31.81 +/- 4.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 684216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017334924 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 3920         |\n",
      "|    policy_gradient_loss | -0.00657     |\n",
      "|    value_loss           | 5.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=685216, episode_reward=32.79 +/- 4.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685216, episode_reward=32.79 +/- 4.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686216, episode_reward=33.36 +/- 2.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686216, episode_reward=33.36 +/- 2.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687216, episode_reward=30.78 +/- 6.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687216, episode_reward=30.78 +/- 6.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688216, episode_reward=34.71 +/- 2.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 688216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019256461 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.33         |\n",
      "|    n_updates            | 3940         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 5.65         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688216, episode_reward=34.71 +/- 2.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 688216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019256461 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.33         |\n",
      "|    n_updates            | 3940         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 5.65         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=689216, episode_reward=31.54 +/- 3.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689216, episode_reward=31.54 +/- 3.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690216, episode_reward=29.06 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690216, episode_reward=29.06 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691216, episode_reward=30.07 +/- 6.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691216, episode_reward=30.07 +/- 6.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692216, episode_reward=31.24 +/- 3.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692216, episode_reward=31.24 +/- 3.89\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 692224   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 692224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693216, episode_reward=32.89 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 693216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019707126 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.55         |\n",
      "|    n_updates            | 3960         |\n",
      "|    policy_gradient_loss | -0.00637     |\n",
      "|    value_loss           | 6.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=693216, episode_reward=32.89 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 32.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 693216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019707126 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.55         |\n",
      "|    n_updates            | 3960         |\n",
      "|    policy_gradient_loss | -0.00637     |\n",
      "|    value_loss           | 6.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=694216, episode_reward=31.58 +/- 3.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694216, episode_reward=31.58 +/- 3.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695216, episode_reward=30.76 +/- 4.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695216, episode_reward=30.76 +/- 4.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696216, episode_reward=32.16 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696216, episode_reward=32.16 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 203      |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 203      |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697216, episode_reward=27.57 +/- 4.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 697216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014688512 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 3980         |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 6.67         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=697216, episode_reward=27.57 +/- 4.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 697216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014688512 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 3980         |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 6.67         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=698216, episode_reward=35.03 +/- 3.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698216, episode_reward=35.03 +/- 3.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699216, episode_reward=32.48 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699216, episode_reward=32.48 +/- 6.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700216, episode_reward=32.35 +/- 3.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700216, episode_reward=32.35 +/- 3.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701216, episode_reward=32.16 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 32.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 701216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002066095 |\n",
      "|    clip_fraction        | 0.0831      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.86        |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    value_loss           | 6.82        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=701216, episode_reward=32.16 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 32.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 701216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002066095 |\n",
      "|    clip_fraction        | 0.0831      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.86        |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    value_loss           | 6.82        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=702216, episode_reward=31.03 +/- 4.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702216, episode_reward=31.03 +/- 4.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=703216, episode_reward=29.32 +/- 3.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=703216, episode_reward=29.32 +/- 3.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704216, episode_reward=29.97 +/- 1.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704216, episode_reward=29.97 +/- 1.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705216, episode_reward=30.40 +/- 6.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 705216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016922688 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 4020         |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    value_loss           | 6.67         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=705216, episode_reward=30.40 +/- 6.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 705216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016922688 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 4020         |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    value_loss           | 6.67         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=706216, episode_reward=29.45 +/- 5.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706216, episode_reward=29.45 +/- 5.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707216, episode_reward=36.58 +/- 1.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 707216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707216, episode_reward=36.58 +/- 1.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 707216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708216, episode_reward=30.50 +/- 5.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708216, episode_reward=30.50 +/- 5.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 708608   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 708608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709216, episode_reward=29.85 +/- 7.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 709216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017790697 |\n",
      "|    clip_fraction        | 0.0747       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.69         |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -0.00685     |\n",
      "|    value_loss           | 6.6          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=709216, episode_reward=29.85 +/- 7.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 709216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017790697 |\n",
      "|    clip_fraction        | 0.0747       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.69         |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -0.00685     |\n",
      "|    value_loss           | 6.6          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=710216, episode_reward=32.17 +/- 7.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710216, episode_reward=32.17 +/- 7.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711216, episode_reward=31.06 +/- 6.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711216, episode_reward=31.06 +/- 6.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712216, episode_reward=35.24 +/- 7.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712216, episode_reward=35.24 +/- 7.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713216, episode_reward=28.88 +/- 1.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 713216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015723285 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 4060         |\n",
      "|    policy_gradient_loss | -0.00653     |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=713216, episode_reward=28.88 +/- 1.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 713216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015723285 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 4060         |\n",
      "|    policy_gradient_loss | -0.00653     |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=714216, episode_reward=32.54 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714216, episode_reward=32.54 +/- 3.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715216, episode_reward=27.70 +/- 4.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 715216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715216, episode_reward=27.70 +/- 4.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 715216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716216, episode_reward=30.89 +/- 4.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716216, episode_reward=30.89 +/- 4.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 716800   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 716800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717216, episode_reward=31.74 +/- 6.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 717216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017038397 |\n",
      "|    clip_fraction        | 0.0728       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 4080         |\n",
      "|    policy_gradient_loss | -0.0079      |\n",
      "|    value_loss           | 6.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=717216, episode_reward=31.74 +/- 6.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 717216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017038397 |\n",
      "|    clip_fraction        | 0.0728       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 4080         |\n",
      "|    policy_gradient_loss | -0.0079      |\n",
      "|    value_loss           | 6.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=718216, episode_reward=31.71 +/- 6.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718216, episode_reward=31.71 +/- 6.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719216, episode_reward=26.94 +/- 4.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 719216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719216, episode_reward=26.94 +/- 4.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 719216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720216, episode_reward=29.35 +/- 7.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720216, episode_reward=29.35 +/- 7.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721216, episode_reward=31.36 +/- 1.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 721216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015496938 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.64         |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.00478     |\n",
      "|    value_loss           | 6.84         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=721216, episode_reward=31.36 +/- 1.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 721216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015496938 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.64         |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.00478     |\n",
      "|    value_loss           | 6.84         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=722216, episode_reward=34.08 +/- 4.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722216, episode_reward=34.08 +/- 4.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=723216, episode_reward=31.73 +/- 1.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=723216, episode_reward=31.73 +/- 1.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724216, episode_reward=34.19 +/- 1.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724216, episode_reward=34.19 +/- 1.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725216, episode_reward=31.77 +/- 7.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015362599 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -0.00685     |\n",
      "|    value_loss           | 6.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=725216, episode_reward=31.77 +/- 7.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015362599 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -0.00685     |\n",
      "|    value_loss           | 6.48         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=726216, episode_reward=25.86 +/- 3.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726216, episode_reward=25.86 +/- 3.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727216, episode_reward=27.03 +/- 4.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727216, episode_reward=27.03 +/- 4.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728216, episode_reward=25.41 +/- 3.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728216, episode_reward=25.41 +/- 3.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 729088   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 729088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729216, episode_reward=29.84 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 729216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017822818 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.41         |\n",
      "|    n_updates            | 4140         |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 6.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=729216, episode_reward=29.84 +/- 4.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 729216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017822818 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.41         |\n",
      "|    n_updates            | 4140         |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 6.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=730216, episode_reward=26.75 +/- 2.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730216, episode_reward=26.75 +/- 2.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731216, episode_reward=25.47 +/- 2.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731216, episode_reward=25.47 +/- 2.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732216, episode_reward=28.01 +/- 6.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732216, episode_reward=28.01 +/- 6.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 733184   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 733184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733216, episode_reward=28.12 +/- 4.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 733216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014658141 |\n",
      "|    clip_fraction        | 0.0669       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.46         |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 6.2          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=733216, episode_reward=28.12 +/- 4.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 733216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014658141 |\n",
      "|    clip_fraction        | 0.0669       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.46         |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 6.2          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=734216, episode_reward=27.43 +/- 6.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734216, episode_reward=27.43 +/- 6.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735216, episode_reward=26.14 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735216, episode_reward=26.14 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736216, episode_reward=25.59 +/- 3.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736216, episode_reward=25.59 +/- 3.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737216, episode_reward=26.64 +/- 4.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737216, episode_reward=26.64 +/- 4.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738216, episode_reward=29.28 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 738216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019545227 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.57         |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | -0.00594     |\n",
      "|    value_loss           | 5.51         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=738216, episode_reward=29.28 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 738216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019545227 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.57         |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | -0.00594     |\n",
      "|    value_loss           | 5.51         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=739216, episode_reward=30.63 +/- 6.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739216, episode_reward=30.63 +/- 6.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740216, episode_reward=30.47 +/- 4.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740216, episode_reward=30.47 +/- 4.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741216, episode_reward=24.07 +/- 0.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741216, episode_reward=24.07 +/- 0.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742216, episode_reward=29.11 +/- 5.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 742216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017911457 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.51         |\n",
      "|    n_updates            | 4200         |\n",
      "|    policy_gradient_loss | -0.00531     |\n",
      "|    value_loss           | 5.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=742216, episode_reward=29.11 +/- 5.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 29.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 742216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017911457 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.51         |\n",
      "|    n_updates            | 4200         |\n",
      "|    policy_gradient_loss | -0.00531     |\n",
      "|    value_loss           | 5.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=743216, episode_reward=32.27 +/- 6.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743216, episode_reward=32.27 +/- 6.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744216, episode_reward=32.10 +/- 4.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744216, episode_reward=32.10 +/- 4.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745216, episode_reward=28.49 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745216, episode_reward=28.49 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=746216, episode_reward=31.80 +/- 4.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 746216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016360644 |\n",
      "|    clip_fraction        | 0.0838       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.73         |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.00703     |\n",
      "|    value_loss           | 5.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=746216, episode_reward=31.80 +/- 4.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 746216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016360644 |\n",
      "|    clip_fraction        | 0.0838       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.73         |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.00703     |\n",
      "|    value_loss           | 5.71         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=747216, episode_reward=27.84 +/- 6.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747216, episode_reward=27.84 +/- 6.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748216, episode_reward=25.58 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748216, episode_reward=25.58 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749216, episode_reward=26.94 +/- 4.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749216, episode_reward=26.94 +/- 4.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 749568   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 749568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750216, episode_reward=30.04 +/- 2.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 750216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015912005 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    value_loss           | 6.49         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=750216, episode_reward=30.04 +/- 2.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 30           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 750216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015912005 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    value_loss           | 6.49         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=751216, episode_reward=28.57 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751216, episode_reward=28.57 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752216, episode_reward=30.32 +/- 7.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 752216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752216, episode_reward=30.32 +/- 7.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 752216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753216, episode_reward=30.88 +/- 5.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753216, episode_reward=30.88 +/- 5.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754216, episode_reward=27.86 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 754216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017970446 |\n",
      "|    clip_fraction        | 0.0741       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.2          |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -0.00655     |\n",
      "|    value_loss           | 6.61         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=754216, episode_reward=27.86 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 754216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017970446 |\n",
      "|    clip_fraction        | 0.0741       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.2          |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -0.00655     |\n",
      "|    value_loss           | 6.61         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=755216, episode_reward=29.56 +/- 3.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755216, episode_reward=29.56 +/- 3.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756216, episode_reward=32.35 +/- 3.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 756216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756216, episode_reward=32.35 +/- 3.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 756216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757216, episode_reward=25.84 +/- 1.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757216, episode_reward=25.84 +/- 1.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 25.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 757760   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 757760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758216, episode_reward=26.56 +/- 6.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 26.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 758216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017164336 |\n",
      "|    clip_fraction        | 0.0699       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.08         |\n",
      "|    n_updates            | 4280         |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    value_loss           | 6.42         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=758216, episode_reward=26.56 +/- 6.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 26.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 758216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017164336 |\n",
      "|    clip_fraction        | 0.0699       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.08         |\n",
      "|    n_updates            | 4280         |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    value_loss           | 6.42         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=759216, episode_reward=31.30 +/- 1.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759216, episode_reward=31.30 +/- 1.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760216, episode_reward=27.43 +/- 8.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760216, episode_reward=27.43 +/- 8.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761216, episode_reward=34.32 +/- 9.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761216, episode_reward=34.32 +/- 9.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762216, episode_reward=24.94 +/- 2.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 24.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 762216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001516104 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.57        |\n",
      "|    n_updates            | 4300        |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 6.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=762216, episode_reward=24.94 +/- 2.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 24.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 762216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001516104 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.57        |\n",
      "|    n_updates            | 4300        |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 6.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=763216, episode_reward=23.87 +/- 3.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763216, episode_reward=23.87 +/- 3.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764216, episode_reward=33.33 +/- 8.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 764216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764216, episode_reward=33.33 +/- 8.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 764216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765216, episode_reward=26.12 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765216, episode_reward=26.12 +/- 2.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 765952   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 765952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766216, episode_reward=26.64 +/- 3.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 26.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018577907 |\n",
      "|    clip_fraction        | 0.0808       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.02         |\n",
      "|    n_updates            | 4320         |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 6.99         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=766216, episode_reward=26.64 +/- 3.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 26.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018577907 |\n",
      "|    clip_fraction        | 0.0808       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.02         |\n",
      "|    n_updates            | 4320         |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 6.99         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=767216, episode_reward=32.77 +/- 7.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767216, episode_reward=32.77 +/- 7.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768216, episode_reward=30.08 +/- 6.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768216, episode_reward=30.08 +/- 6.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=769216, episode_reward=29.92 +/- 7.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=769216, episode_reward=29.92 +/- 7.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770216, episode_reward=31.82 +/- 9.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 770216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017221302 |\n",
      "|    clip_fraction        | 0.0787       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.26         |\n",
      "|    n_updates            | 4340         |\n",
      "|    policy_gradient_loss | -0.00668     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=770216, episode_reward=31.82 +/- 9.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 770216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017221302 |\n",
      "|    clip_fraction        | 0.0787       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.26         |\n",
      "|    n_updates            | 4340         |\n",
      "|    policy_gradient_loss | -0.00668     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=771216, episode_reward=26.13 +/- 5.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=771216, episode_reward=26.13 +/- 5.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772216, episode_reward=33.08 +/- 8.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772216, episode_reward=33.08 +/- 8.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773216, episode_reward=32.95 +/- 9.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773216, episode_reward=32.95 +/- 9.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3424     |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774216, episode_reward=37.32 +/- 7.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 774216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017738875 |\n",
      "|    clip_fraction        | 0.0712       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.62         |\n",
      "|    n_updates            | 4360         |\n",
      "|    policy_gradient_loss | -0.00519     |\n",
      "|    value_loss           | 5.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=774216, episode_reward=37.32 +/- 7.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 37.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 774216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017738875 |\n",
      "|    clip_fraction        | 0.0712       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.62         |\n",
      "|    n_updates            | 4360         |\n",
      "|    policy_gradient_loss | -0.00519     |\n",
      "|    value_loss           | 5.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=775216, episode_reward=39.46 +/- 9.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775216, episode_reward=39.46 +/- 9.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776216, episode_reward=34.39 +/- 11.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776216, episode_reward=34.39 +/- 11.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777216, episode_reward=34.25 +/- 11.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777216, episode_reward=34.25 +/- 11.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778216, episode_reward=29.36 +/- 8.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778216, episode_reward=29.36 +/- 8.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 778240   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 778240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779216, episode_reward=27.52 +/- 3.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 779216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017005149 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 4380         |\n",
      "|    policy_gradient_loss | -0.00654     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=779216, episode_reward=27.52 +/- 3.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 27.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 779216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017005149 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 4380         |\n",
      "|    policy_gradient_loss | -0.00654     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=780216, episode_reward=28.86 +/- 4.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780216, episode_reward=28.86 +/- 4.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781216, episode_reward=31.17 +/- 4.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781216, episode_reward=31.17 +/- 4.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782216, episode_reward=27.38 +/- 8.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782216, episode_reward=27.38 +/- 8.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 782336   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 782336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=783216, episode_reward=30.37 +/- 6.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 30.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 783216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001794538 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    value_loss           | 5.3         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=783216, episode_reward=30.37 +/- 6.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 30.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 783216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001794538 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    value_loss           | 5.3         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=784216, episode_reward=27.19 +/- 6.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784216, episode_reward=27.19 +/- 6.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 27.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785216, episode_reward=33.83 +/- 7.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785216, episode_reward=33.83 +/- 7.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786216, episode_reward=28.42 +/- 7.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786216, episode_reward=28.42 +/- 7.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 28.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787216, episode_reward=27.73 +/- 6.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 27.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 787216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001706474 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    value_loss           | 6.01        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=787216, episode_reward=27.73 +/- 6.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 27.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 787216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001706474 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    value_loss           | 6.01        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=788216, episode_reward=31.61 +/- 7.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788216, episode_reward=31.61 +/- 7.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 31.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789216, episode_reward=30.65 +/- 7.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789216, episode_reward=30.65 +/- 7.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790216, episode_reward=30.14 +/- 3.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790216, episode_reward=30.14 +/- 3.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=791216, episode_reward=28.11 +/- 8.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 28.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 791216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001864271 |\n",
      "|    clip_fraction        | 0.0755      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.83        |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 6.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=791216, episode_reward=28.11 +/- 8.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 28.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 791216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001864271 |\n",
      "|    clip_fraction        | 0.0755      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.83        |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 6.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=792216, episode_reward=32.46 +/- 7.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792216, episode_reward=32.46 +/- 7.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793216, episode_reward=36.35 +/- 3.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 793216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793216, episode_reward=36.35 +/- 3.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 793216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794216, episode_reward=30.23 +/- 8.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794216, episode_reward=30.23 +/- 8.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 794624   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 794624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795216, episode_reward=41.06 +/- 10.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 795216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018769039 |\n",
      "|    clip_fraction        | 0.0682       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.59         |\n",
      "|    n_updates            | 4460         |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    value_loss           | 5.49         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=795216, episode_reward=41.06 +/- 10.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 41.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 795216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018769039 |\n",
      "|    clip_fraction        | 0.0682       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.59         |\n",
      "|    n_updates            | 4460         |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    value_loss           | 5.49         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=796216, episode_reward=43.32 +/- 6.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796216, episode_reward=43.32 +/- 6.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797216, episode_reward=42.17 +/- 3.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797216, episode_reward=42.17 +/- 3.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798216, episode_reward=38.48 +/- 1.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798216, episode_reward=38.48 +/- 1.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799216, episode_reward=42.59 +/- 2.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 799216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014986687 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.66         |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.00537     |\n",
      "|    value_loss           | 6.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=799216, episode_reward=42.59 +/- 2.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 799216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014986687 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.66         |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.00537     |\n",
      "|    value_loss           | 6.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=800216, episode_reward=38.31 +/- 5.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800216, episode_reward=38.31 +/- 5.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801216, episode_reward=41.21 +/- 10.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 801216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801216, episode_reward=41.21 +/- 10.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 801216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802216, episode_reward=40.97 +/- 8.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802216, episode_reward=40.97 +/- 8.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=803216, episode_reward=35.30 +/- 10.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 803216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019353225 |\n",
      "|    clip_fraction        | 0.081        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 4500         |\n",
      "|    policy_gradient_loss | -0.00755     |\n",
      "|    value_loss           | 5.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=803216, episode_reward=35.30 +/- 10.65\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 35.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 803216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019353225 |\n",
      "|    clip_fraction        | 0.081        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 4500         |\n",
      "|    policy_gradient_loss | -0.00755     |\n",
      "|    value_loss           | 5.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=804216, episode_reward=34.47 +/- 5.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804216, episode_reward=34.47 +/- 5.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805216, episode_reward=35.68 +/- 3.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 805216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805216, episode_reward=35.68 +/- 3.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 805216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806216, episode_reward=38.06 +/- 6.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806216, episode_reward=38.06 +/- 6.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 806912   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 806912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807216, episode_reward=28.79 +/- 10.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 807216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013961806 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 4520         |\n",
      "|    policy_gradient_loss | -0.00666     |\n",
      "|    value_loss           | 5.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=807216, episode_reward=28.79 +/- 10.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 28.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 807216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013961806 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 4520         |\n",
      "|    policy_gradient_loss | -0.00666     |\n",
      "|    value_loss           | 5.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=808216, episode_reward=35.67 +/- 9.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808216, episode_reward=35.67 +/- 9.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809216, episode_reward=37.39 +/- 12.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809216, episode_reward=37.39 +/- 12.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810216, episode_reward=32.71 +/- 8.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810216, episode_reward=32.71 +/- 8.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 32.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811216, episode_reward=36.64 +/- 8.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 811216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018312007 |\n",
      "|    clip_fraction        | 0.0845       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | -0.00653     |\n",
      "|    value_loss           | 5.88         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=811216, episode_reward=36.64 +/- 8.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 36.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 811216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018312007 |\n",
      "|    clip_fraction        | 0.0845       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | -0.00653     |\n",
      "|    value_loss           | 5.88         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=812216, episode_reward=29.82 +/- 7.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812216, episode_reward=29.82 +/- 7.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 29.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813216, episode_reward=35.76 +/- 6.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813216, episode_reward=35.76 +/- 6.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814216, episode_reward=36.98 +/- 10.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814216, episode_reward=36.98 +/- 10.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 815104   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 815104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815216, episode_reward=42.33 +/- 0.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 815216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020158961 |\n",
      "|    clip_fraction        | 0.0727       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.47         |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -0.00722     |\n",
      "|    value_loss           | 6.29         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=815216, episode_reward=42.33 +/- 0.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 815216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020158961 |\n",
      "|    clip_fraction        | 0.0727       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.47         |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -0.00722     |\n",
      "|    value_loss           | 6.29         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=816216, episode_reward=38.60 +/- 5.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816216, episode_reward=38.60 +/- 5.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817216, episode_reward=33.91 +/- 9.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817216, episode_reward=33.91 +/- 9.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818216, episode_reward=35.26 +/- 8.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818216, episode_reward=35.26 +/- 8.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819216, episode_reward=39.36 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 819216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016634379 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 4580         |\n",
      "|    policy_gradient_loss | -0.00618     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=819216, episode_reward=39.36 +/- 4.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 819216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016634379 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 4580         |\n",
      "|    policy_gradient_loss | -0.00618     |\n",
      "|    value_loss           | 6            |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=820216, episode_reward=34.23 +/- 6.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820216, episode_reward=34.23 +/- 6.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 34.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821216, episode_reward=41.69 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821216, episode_reward=41.69 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822216, episode_reward=41.33 +/- 2.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822216, episode_reward=41.33 +/- 2.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823216, episode_reward=37.52 +/- 7.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823216, episode_reward=37.52 +/- 7.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824216, episode_reward=38.10 +/- 8.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 824216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015954738 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 4600         |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 5.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=824216, episode_reward=38.10 +/- 8.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 824216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015954738 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 4600         |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 5.77         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=825216, episode_reward=46.75 +/- 3.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825216, episode_reward=46.75 +/- 3.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826216, episode_reward=38.17 +/- 8.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826216, episode_reward=38.17 +/- 8.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 38.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827216, episode_reward=36.62 +/- 5.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827216, episode_reward=36.62 +/- 5.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828216, episode_reward=44.89 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 828216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016930231 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.15         |\n",
      "|    n_updates            | 4620         |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    value_loss           | 5.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=828216, episode_reward=44.89 +/- 4.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 828216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016930231 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.15         |\n",
      "|    n_updates            | 4620         |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    value_loss           | 5.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=829216, episode_reward=44.35 +/- 3.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829216, episode_reward=44.35 +/- 3.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 44.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830216, episode_reward=46.16 +/- 4.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830216, episode_reward=46.16 +/- 4.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831216, episode_reward=41.36 +/- 1.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831216, episode_reward=41.36 +/- 1.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3423     |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832216, episode_reward=41.01 +/- 3.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 41          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 832216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001959906 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.96        |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 6.47        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=832216, episode_reward=41.01 +/- 3.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 41          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 832216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001959906 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.96        |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 6.47        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=833216, episode_reward=39.37 +/- 1.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833216, episode_reward=39.37 +/- 1.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834216, episode_reward=41.22 +/- 3.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834216, episode_reward=41.22 +/- 3.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835216, episode_reward=40.24 +/- 1.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835216, episode_reward=40.24 +/- 1.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836216, episode_reward=38.68 +/- 8.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 836216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020184056 |\n",
      "|    clip_fraction        | 0.0791       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 4660         |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    value_loss           | 5.98         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=836216, episode_reward=38.68 +/- 8.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 38.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 836216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020184056 |\n",
      "|    clip_fraction        | 0.0791       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 4660         |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    value_loss           | 5.98         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=837216, episode_reward=40.10 +/- 4.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837216, episode_reward=40.10 +/- 4.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838216, episode_reward=43.05 +/- 3.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 838216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838216, episode_reward=43.05 +/- 3.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 838216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839216, episode_reward=41.42 +/- 3.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839216, episode_reward=41.42 +/- 3.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 839680   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 839680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840216, episode_reward=43.04 +/- 2.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015968909 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 4680         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 5.64         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=840216, episode_reward=43.04 +/- 2.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 43           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015968909 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.99         |\n",
      "|    n_updates            | 4680         |\n",
      "|    policy_gradient_loss | -0.00681     |\n",
      "|    value_loss           | 5.64         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=841216, episode_reward=42.34 +/- 2.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841216, episode_reward=42.34 +/- 2.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842216, episode_reward=42.36 +/- 3.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 842216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842216, episode_reward=42.36 +/- 3.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 842216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843216, episode_reward=42.72 +/- 3.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843216, episode_reward=42.72 +/- 3.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 246      |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 246      |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844216, episode_reward=42.61 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 844216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017936251 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 4700         |\n",
      "|    policy_gradient_loss | -0.00706     |\n",
      "|    value_loss           | 6.22         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=844216, episode_reward=42.61 +/- 2.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 844216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017936251 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 4700         |\n",
      "|    policy_gradient_loss | -0.00706     |\n",
      "|    value_loss           | 6.22         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=845216, episode_reward=41.72 +/- 2.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845216, episode_reward=41.72 +/- 2.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846216, episode_reward=43.90 +/- 3.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846216, episode_reward=43.90 +/- 3.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847216, episode_reward=42.56 +/- 2.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847216, episode_reward=42.56 +/- 2.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848216, episode_reward=42.13 +/- 3.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 848216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017254386 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 4720         |\n",
      "|    policy_gradient_loss | -0.00713     |\n",
      "|    value_loss           | 6.79         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=848216, episode_reward=42.13 +/- 3.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 42.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 848216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017254386 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 4720         |\n",
      "|    policy_gradient_loss | -0.00713     |\n",
      "|    value_loss           | 6.79         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=849216, episode_reward=41.72 +/- 2.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849216, episode_reward=41.72 +/- 2.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 41.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850216, episode_reward=46.38 +/- 5.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850216, episode_reward=46.38 +/- 5.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 46.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851216, episode_reward=43.62 +/- 4.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851216, episode_reward=43.62 +/- 4.88\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852216, episode_reward=44.04 +/- 6.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 852216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013649618 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.68         |\n",
      "|    n_updates            | 4740         |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 6.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=852216, episode_reward=44.04 +/- 6.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 44           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 852216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013649618 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.68         |\n",
      "|    n_updates            | 4740         |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 6.41         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=853216, episode_reward=42.81 +/- 3.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853216, episode_reward=42.81 +/- 3.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854216, episode_reward=40.25 +/- 1.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854216, episode_reward=40.25 +/- 1.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855216, episode_reward=40.59 +/- 2.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855216, episode_reward=40.59 +/- 2.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 856064   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3422     |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 856064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856216, episode_reward=33.31 +/- 2.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 856216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019208449 |\n",
      "|    clip_fraction        | 0.0711       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.61         |\n",
      "|    n_updates            | 4760         |\n",
      "|    policy_gradient_loss | -0.00604     |\n",
      "|    value_loss           | 5.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=856216, episode_reward=33.31 +/- 2.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 33.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 856216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019208449 |\n",
      "|    clip_fraction        | 0.0711       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 4.61         |\n",
      "|    n_updates            | 4760         |\n",
      "|    policy_gradient_loss | -0.00604     |\n",
      "|    value_loss           | 5.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=857216, episode_reward=39.10 +/- 4.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=857216, episode_reward=39.10 +/- 4.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858216, episode_reward=40.50 +/- 8.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858216, episode_reward=40.50 +/- 8.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 40.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859216, episode_reward=37.63 +/- 5.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859216, episode_reward=37.63 +/- 5.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3421     |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3421     |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860216, episode_reward=33.33 +/- 14.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 33.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002083774 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.52        |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.00823    |\n",
      "|    value_loss           | 5.45        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860216, episode_reward=33.33 +/- 14.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 33.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002083774 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.52        |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.00823    |\n",
      "|    value_loss           | 5.45        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=861216, episode_reward=36.68 +/- 9.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861216, episode_reward=36.68 +/- 9.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862216, episode_reward=42.47 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862216, episode_reward=42.47 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863216, episode_reward=39.59 +/- 7.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863216, episode_reward=39.59 +/- 7.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864216, episode_reward=42.65 +/- 3.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864216, episode_reward=42.65 +/- 3.68\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3421     |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 864256   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3421     |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 864256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865216, episode_reward=39.14 +/- 8.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 865216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018143042 |\n",
      "|    clip_fraction        | 0.0777       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.57         |\n",
      "|    n_updates            | 4800         |\n",
      "|    policy_gradient_loss | -0.00601     |\n",
      "|    value_loss           | 6.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=865216, episode_reward=39.14 +/- 8.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 39.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 865216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018143042 |\n",
      "|    clip_fraction        | 0.0777       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.57         |\n",
      "|    n_updates            | 4800         |\n",
      "|    policy_gradient_loss | -0.00601     |\n",
      "|    value_loss           | 6.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=866216, episode_reward=35.35 +/- 10.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866216, episode_reward=35.35 +/- 10.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 35.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867216, episode_reward=43.30 +/- 4.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867216, episode_reward=43.30 +/- 4.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 43.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868216, episode_reward=42.85 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868216, episode_reward=42.85 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 42.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3421     |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3421     |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=869216, episode_reward=34.56 +/- 9.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 869216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017823558 |\n",
      "|    clip_fraction        | 0.0749       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 4820         |\n",
      "|    policy_gradient_loss | -0.00754     |\n",
      "|    value_loss           | 5.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=869216, episode_reward=34.56 +/- 9.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 34.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 869216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017823558 |\n",
      "|    clip_fraction        | 0.0749       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 4820         |\n",
      "|    policy_gradient_loss | -0.00754     |\n",
      "|    value_loss           | 5.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=870216, episode_reward=33.18 +/- 8.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870216, episode_reward=33.18 +/- 8.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 33.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=871216, episode_reward=29.95 +/- 8.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=871216, episode_reward=29.95 +/- 8.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 30       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872216, episode_reward=39.02 +/- 7.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872216, episode_reward=39.02 +/- 7.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873216, episode_reward=46.01 +/- 15.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 46         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 873216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00141093 |\n",
      "|    clip_fraction        | 0.0658     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.44      |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 1.9        |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00543   |\n",
      "|    value_loss           | 4.9        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=873216, episode_reward=46.01 +/- 15.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 46         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 873216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00141093 |\n",
      "|    clip_fraction        | 0.0658     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.44      |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 1.9        |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00543   |\n",
      "|    value_loss           | 4.9        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=874216, episode_reward=49.01 +/- 23.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874216, episode_reward=49.01 +/- 23.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 49       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875216, episode_reward=52.65 +/- 11.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875216, episode_reward=52.65 +/- 11.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 52.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876216, episode_reward=57.21 +/- 13.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 57.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876216, episode_reward=57.21 +/- 13.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 57.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=877216, episode_reward=88.88 +/- 6.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 88.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 877216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002122878 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.76        |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    value_loss           | 6.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=877216, episode_reward=88.88 +/- 6.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 88.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 877216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002122878 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.76        |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    value_loss           | 6.22        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=878216, episode_reward=67.47 +/- 13.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 67.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878216   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=878216, episode_reward=67.47 +/- 13.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 67.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879216, episode_reward=83.65 +/- 10.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 83.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 879216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879216, episode_reward=83.65 +/- 10.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 83.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 879216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880216, episode_reward=76.02 +/- 3.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880216, episode_reward=76.02 +/- 3.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=881216, episode_reward=75.81 +/- 2.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 75.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 881216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002297441 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.43       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    value_loss           | 5.97        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=881216, episode_reward=75.81 +/- 2.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 75.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 881216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002297441 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.43       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    value_loss           | 5.97        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=882216, episode_reward=73.96 +/- 2.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882216, episode_reward=73.96 +/- 2.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883216, episode_reward=82.77 +/- 8.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883216, episode_reward=82.77 +/- 8.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884216, episode_reward=75.72 +/- 2.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884216, episode_reward=75.72 +/- 2.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885216, episode_reward=86.21 +/- 11.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 86.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 885216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018994184 |\n",
      "|    clip_fraction        | 0.084        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 4900         |\n",
      "|    policy_gradient_loss | -0.00679     |\n",
      "|    value_loss           | 5.52         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=885216, episode_reward=86.21 +/- 11.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 86.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 885216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018994184 |\n",
      "|    clip_fraction        | 0.084        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 4900         |\n",
      "|    policy_gradient_loss | -0.00679     |\n",
      "|    value_loss           | 5.52         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=886216, episode_reward=81.75 +/- 8.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886216, episode_reward=81.75 +/- 8.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887216, episode_reward=82.24 +/- 9.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 887216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887216, episode_reward=82.24 +/- 9.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 887216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888216, episode_reward=81.99 +/- 5.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888216, episode_reward=81.99 +/- 5.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 888832   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 888832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=889216, episode_reward=82.14 +/- 8.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 889216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017671303 |\n",
      "|    clip_fraction        | 0.0759       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.75         |\n",
      "|    n_updates            | 4920         |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 6.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=889216, episode_reward=82.14 +/- 8.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 889216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017671303 |\n",
      "|    clip_fraction        | 0.0759       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.75         |\n",
      "|    n_updates            | 4920         |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 6.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=890216, episode_reward=74.54 +/- 4.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890216, episode_reward=74.54 +/- 4.51\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891216, episode_reward=76.34 +/- 5.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 891216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891216, episode_reward=76.34 +/- 5.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 891216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892216, episode_reward=76.90 +/- 5.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892216, episode_reward=76.90 +/- 5.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 261      |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 261      |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=893216, episode_reward=75.43 +/- 2.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 75.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 893216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020092728 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.66         |\n",
      "|    n_updates            | 4940         |\n",
      "|    policy_gradient_loss | -0.00702     |\n",
      "|    value_loss           | 5.89         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=893216, episode_reward=75.43 +/- 2.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 75.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 893216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020092728 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.66         |\n",
      "|    n_updates            | 4940         |\n",
      "|    policy_gradient_loss | -0.00702     |\n",
      "|    value_loss           | 5.89         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=894216, episode_reward=81.13 +/- 8.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894216, episode_reward=81.13 +/- 8.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895216, episode_reward=85.46 +/- 4.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895216, episode_reward=85.46 +/- 4.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896216, episode_reward=76.00 +/- 3.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896216, episode_reward=76.00 +/- 3.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897216, episode_reward=81.09 +/- 9.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 81.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 897216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017872226 |\n",
      "|    clip_fraction        | 0.0774       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 4960         |\n",
      "|    policy_gradient_loss | -0.00665     |\n",
      "|    value_loss           | 5.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=897216, episode_reward=81.09 +/- 9.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 81.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 897216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017872226 |\n",
      "|    clip_fraction        | 0.0774       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 4960         |\n",
      "|    policy_gradient_loss | -0.00665     |\n",
      "|    value_loss           | 5.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=898216, episode_reward=78.99 +/- 8.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898216, episode_reward=78.99 +/- 8.72\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899216, episode_reward=77.13 +/- 6.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899216, episode_reward=77.13 +/- 6.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900216, episode_reward=86.03 +/- 10.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 86       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900216, episode_reward=86.03 +/- 10.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 86       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901216, episode_reward=79.22 +/- 9.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 79.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 901216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019587567 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.65         |\n",
      "|    n_updates            | 4980         |\n",
      "|    policy_gradient_loss | -0.00693     |\n",
      "|    value_loss           | 5.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=901216, episode_reward=79.22 +/- 9.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 79.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 901216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019587567 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.65         |\n",
      "|    n_updates            | 4980         |\n",
      "|    policy_gradient_loss | -0.00693     |\n",
      "|    value_loss           | 5.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=902216, episode_reward=79.93 +/- 8.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902216, episode_reward=79.93 +/- 8.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903216, episode_reward=78.32 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903216, episode_reward=78.32 +/- 6.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904216, episode_reward=80.56 +/- 9.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904216, episode_reward=80.56 +/- 9.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905216, episode_reward=82.77 +/- 9.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905216, episode_reward=82.77 +/- 9.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906216, episode_reward=86.98 +/- 9.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 87           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 906216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016589116 |\n",
      "|    clip_fraction        | 0.0702       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.98         |\n",
      "|    n_updates            | 5000         |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 5.03         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=906216, episode_reward=86.98 +/- 9.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 87           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 906216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016589116 |\n",
      "|    clip_fraction        | 0.0702       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.98         |\n",
      "|    n_updates            | 5000         |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 5.03         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=907216, episode_reward=72.96 +/- 1.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 73       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907216, episode_reward=72.96 +/- 1.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 73       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908216, episode_reward=76.62 +/- 8.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908216, episode_reward=76.62 +/- 8.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909216, episode_reward=80.93 +/- 7.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909216, episode_reward=80.93 +/- 7.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 265      |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 265      |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910216, episode_reward=82.42 +/- 9.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 910216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016754563 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.78         |\n",
      "|    n_updates            | 5020         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 6.45         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=910216, episode_reward=82.42 +/- 9.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 910216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016754563 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.78         |\n",
      "|    n_updates            | 5020         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 6.45         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=911216, episode_reward=78.59 +/- 7.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911216, episode_reward=78.59 +/- 7.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912216, episode_reward=87.02 +/- 11.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 87       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912216, episode_reward=87.02 +/- 11.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 87       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913216, episode_reward=79.51 +/- 6.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913216, episode_reward=79.51 +/- 6.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 913408   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 913408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914216, episode_reward=72.95 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 914216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001688902 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.6         |\n",
      "|    n_updates            | 5040        |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 6.41        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=914216, episode_reward=72.95 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 914216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001688902 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.6         |\n",
      "|    n_updates            | 5040        |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 6.41        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=915216, episode_reward=78.66 +/- 12.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915216, episode_reward=78.66 +/- 12.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916216, episode_reward=79.19 +/- 12.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916216, episode_reward=79.19 +/- 12.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917216, episode_reward=79.15 +/- 7.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917216, episode_reward=79.15 +/- 7.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918216, episode_reward=78.42 +/- 9.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 78.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 918216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018281596 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.43         |\n",
      "|    n_updates            | 5060         |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 5.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=918216, episode_reward=78.42 +/- 9.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 78.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 918216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018281596 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.43         |\n",
      "|    n_updates            | 5060         |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 5.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=919216, episode_reward=78.05 +/- 6.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919216, episode_reward=78.05 +/- 6.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920216, episode_reward=75.94 +/- 5.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920216, episode_reward=75.94 +/- 5.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921216, episode_reward=88.15 +/- 9.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 88.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921216, episode_reward=88.15 +/- 9.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 88.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 269      |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 269      |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922216, episode_reward=78.60 +/- 5.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 78.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 922216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017350925 |\n",
      "|    clip_fraction        | 0.0752       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.13         |\n",
      "|    n_updates            | 5080         |\n",
      "|    policy_gradient_loss | -0.0069      |\n",
      "|    value_loss           | 5.62         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=922216, episode_reward=78.60 +/- 5.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 78.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 922216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017350925 |\n",
      "|    clip_fraction        | 0.0752       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.13         |\n",
      "|    n_updates            | 5080         |\n",
      "|    policy_gradient_loss | -0.0069      |\n",
      "|    value_loss           | 5.62         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=923216, episode_reward=75.45 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923216, episode_reward=75.45 +/- 3.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924216, episode_reward=77.97 +/- 7.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 924216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924216, episode_reward=77.97 +/- 7.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 924216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925216, episode_reward=80.44 +/- 7.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925216, episode_reward=80.44 +/- 7.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926216, episode_reward=73.55 +/- 2.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 73.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 926216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015097074 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.13         |\n",
      "|    n_updates            | 5100         |\n",
      "|    policy_gradient_loss | -0.00576     |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=926216, episode_reward=73.55 +/- 2.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 73.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 926216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015097074 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.13         |\n",
      "|    n_updates            | 5100         |\n",
      "|    policy_gradient_loss | -0.00576     |\n",
      "|    value_loss           | 6.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=927216, episode_reward=78.30 +/- 10.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927216, episode_reward=78.30 +/- 10.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928216, episode_reward=83.57 +/- 4.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 83.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 928216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928216, episode_reward=83.57 +/- 4.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 83.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 928216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929216, episode_reward=74.49 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929216, episode_reward=74.49 +/- 4.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 929792   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3420     |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 929792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930216, episode_reward=78.60 +/- 7.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 78.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017496026 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 5120         |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    value_loss           | 6.07         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=930216, episode_reward=78.60 +/- 7.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 78.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017496026 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 5120         |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    value_loss           | 6.07         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=931216, episode_reward=81.73 +/- 10.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931216, episode_reward=81.73 +/- 10.81\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932216, episode_reward=74.40 +/- 2.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932216, episode_reward=74.40 +/- 2.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933216, episode_reward=82.88 +/- 10.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933216, episode_reward=82.88 +/- 10.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934216, episode_reward=82.70 +/- 8.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 934216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018654699 |\n",
      "|    clip_fraction        | 0.0759       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 5140         |\n",
      "|    policy_gradient_loss | -0.00743     |\n",
      "|    value_loss           | 5.73         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=934216, episode_reward=82.70 +/- 8.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 934216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018654699 |\n",
      "|    clip_fraction        | 0.0759       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 5140         |\n",
      "|    policy_gradient_loss | -0.00743     |\n",
      "|    value_loss           | 5.73         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=935216, episode_reward=79.42 +/- 6.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935216, episode_reward=79.42 +/- 6.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936216, episode_reward=82.88 +/- 11.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 936216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936216, episode_reward=82.88 +/- 11.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 936216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937216, episode_reward=75.09 +/- 5.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937216, episode_reward=75.09 +/- 5.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 937984   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 937984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938216, episode_reward=76.59 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 76.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 938216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018445216 |\n",
      "|    clip_fraction        | 0.0723       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.64         |\n",
      "|    n_updates            | 5160         |\n",
      "|    policy_gradient_loss | -0.00679     |\n",
      "|    value_loss           | 5.56         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=938216, episode_reward=76.59 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 76.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 938216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018445216 |\n",
      "|    clip_fraction        | 0.0723       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.64         |\n",
      "|    n_updates            | 5160         |\n",
      "|    policy_gradient_loss | -0.00679     |\n",
      "|    value_loss           | 5.56         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=939216, episode_reward=75.08 +/- 3.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939216, episode_reward=75.08 +/- 3.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940216, episode_reward=80.84 +/- 8.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940216, episode_reward=80.84 +/- 8.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941216, episode_reward=76.90 +/- 5.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941216, episode_reward=76.90 +/- 5.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942216, episode_reward=84.35 +/- 9.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 84.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 942216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018978799 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.21         |\n",
      "|    n_updates            | 5180         |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 6.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=942216, episode_reward=84.35 +/- 9.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 84.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 942216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018978799 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.21         |\n",
      "|    n_updates            | 5180         |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 6.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=943216, episode_reward=78.35 +/- 6.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943216, episode_reward=78.35 +/- 6.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944216, episode_reward=77.52 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944216, episode_reward=77.52 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945216, episode_reward=76.16 +/- 3.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945216, episode_reward=76.16 +/- 3.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946216, episode_reward=82.11 +/- 7.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 946216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021001839 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.76         |\n",
      "|    n_updates            | 5200         |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 5.57         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=946216, episode_reward=82.11 +/- 7.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 82.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 946216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021001839 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.76         |\n",
      "|    n_updates            | 5200         |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 5.57         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=947216, episode_reward=78.73 +/- 8.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947216, episode_reward=78.73 +/- 8.83\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948216, episode_reward=75.02 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948216, episode_reward=75.02 +/- 4.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949216, episode_reward=81.16 +/- 11.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949216, episode_reward=81.16 +/- 11.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950216, episode_reward=77.97 +/- 5.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950216, episode_reward=77.97 +/- 5.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951216, episode_reward=76.97 +/- 0.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 77           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 951216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018255108 |\n",
      "|    clip_fraction        | 0.063        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.44        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.41         |\n",
      "|    n_updates            | 5220         |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 6.53         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=951216, episode_reward=76.97 +/- 0.99\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 77           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 951216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018255108 |\n",
      "|    clip_fraction        | 0.063        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.44        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.41         |\n",
      "|    n_updates            | 5220         |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 6.53         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=952216, episode_reward=87.68 +/- 12.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 87.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952216, episode_reward=87.68 +/- 12.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 87.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953216, episode_reward=75.16 +/- 3.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953216, episode_reward=75.16 +/- 3.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954216, episode_reward=81.80 +/- 11.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954216, episode_reward=81.80 +/- 11.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 954368   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3419     |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 954368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955216, episode_reward=77.11 +/- 2.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 77.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 955216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001991476 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.96        |\n",
      "|    n_updates            | 5240        |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=955216, episode_reward=77.11 +/- 2.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | 77.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 955216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001991476 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 2.96        |\n",
      "|    n_updates            | 5240        |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=956216, episode_reward=82.64 +/- 8.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956216, episode_reward=82.64 +/- 8.69\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957216, episode_reward=78.13 +/- 2.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957216, episode_reward=78.13 +/- 2.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958216, episode_reward=78.10 +/- 5.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958216, episode_reward=78.10 +/- 5.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959216, episode_reward=80.18 +/- 9.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 80.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 959216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018575954 |\n",
      "|    clip_fraction        | 0.0786       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.58         |\n",
      "|    n_updates            | 5260         |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    value_loss           | 6.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=959216, episode_reward=80.18 +/- 9.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 80.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 959216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018575954 |\n",
      "|    clip_fraction        | 0.0786       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.58         |\n",
      "|    n_updates            | 5260         |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    value_loss           | 6.4          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=960216, episode_reward=79.81 +/- 7.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960216, episode_reward=79.81 +/- 7.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961216, episode_reward=73.62 +/- 2.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 73.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 961216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961216, episode_reward=73.62 +/- 2.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 73.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 961216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962216, episode_reward=77.38 +/- 7.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962216, episode_reward=77.38 +/- 7.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 962560   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 962560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963216, episode_reward=85.31 +/- 7.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 85.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 963216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016121342 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.36         |\n",
      "|    n_updates            | 5280         |\n",
      "|    policy_gradient_loss | -0.00587     |\n",
      "|    value_loss           | 5.51         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=963216, episode_reward=85.31 +/- 7.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 85.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 963216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016121342 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.36         |\n",
      "|    n_updates            | 5280         |\n",
      "|    policy_gradient_loss | -0.00587     |\n",
      "|    value_loss           | 5.51         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=964216, episode_reward=78.67 +/- 10.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964216, episode_reward=78.67 +/- 10.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965216, episode_reward=85.09 +/- 14.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 965216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965216, episode_reward=85.09 +/- 14.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 965216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966216, episode_reward=76.39 +/- 2.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966216, episode_reward=76.39 +/- 2.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967216, episode_reward=89.83 +/- 11.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 89.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 967216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019364017 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.68         |\n",
      "|    n_updates            | 5300         |\n",
      "|    policy_gradient_loss | -0.00688     |\n",
      "|    value_loss           | 6.33         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=967216, episode_reward=89.83 +/- 11.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 89.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 967216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019364017 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.68         |\n",
      "|    n_updates            | 5300         |\n",
      "|    policy_gradient_loss | -0.00688     |\n",
      "|    value_loss           | 6.33         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=968216, episode_reward=81.61 +/- 8.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968216   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=968216, episode_reward=81.61 +/- 8.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969216, episode_reward=85.07 +/- 11.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969216, episode_reward=85.07 +/- 11.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970216, episode_reward=76.72 +/- 3.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970216, episode_reward=76.72 +/- 3.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971216, episode_reward=74.74 +/- 2.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 74.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 971216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00193498 |\n",
      "|    clip_fraction        | 0.0673     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.5       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 2.67       |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00634   |\n",
      "|    value_loss           | 5.92       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=971216, episode_reward=74.74 +/- 2.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 74.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 971216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00193498 |\n",
      "|    clip_fraction        | 0.0673     |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -2.5       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 2.67       |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00634   |\n",
      "|    value_loss           | 5.92       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=972216, episode_reward=72.71 +/- 1.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972216, episode_reward=72.71 +/- 1.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973216, episode_reward=76.39 +/- 4.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 973216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973216, episode_reward=76.39 +/- 4.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 973216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974216, episode_reward=78.31 +/- 6.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974216, episode_reward=78.31 +/- 6.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 974848   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 974848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975216, episode_reward=80.80 +/- 11.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 80.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 975216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017136628 |\n",
      "|    clip_fraction        | 0.0642       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 5340         |\n",
      "|    policy_gradient_loss | -0.00607     |\n",
      "|    value_loss           | 5.83         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=975216, episode_reward=80.80 +/- 11.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 80.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 975216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017136628 |\n",
      "|    clip_fraction        | 0.0642       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 5340         |\n",
      "|    policy_gradient_loss | -0.00607     |\n",
      "|    value_loss           | 5.83         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=976216, episode_reward=85.56 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976216, episode_reward=85.56 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977216, episode_reward=85.05 +/- 11.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 977216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977216, episode_reward=85.05 +/- 11.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 85       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 977216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978216, episode_reward=81.23 +/- 6.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978216, episode_reward=81.23 +/- 6.73\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 978944   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 978944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979216, episode_reward=85.71 +/- 7.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 85.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 979216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018936467 |\n",
      "|    clip_fraction        | 0.0683       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 5360         |\n",
      "|    policy_gradient_loss | -0.0069      |\n",
      "|    value_loss           | 5.57         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=979216, episode_reward=85.71 +/- 7.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 85.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 979216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018936467 |\n",
      "|    clip_fraction        | 0.0683       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 5360         |\n",
      "|    policy_gradient_loss | -0.0069      |\n",
      "|    value_loss           | 5.57         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=980216, episode_reward=77.37 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980216, episode_reward=77.37 +/- 6.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981216, episode_reward=72.81 +/- 1.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 72.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981216, episode_reward=72.81 +/- 1.96\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 72.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982216, episode_reward=74.27 +/- 2.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982216, episode_reward=74.27 +/- 2.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983216, episode_reward=92.96 +/- 9.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 93           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 983216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018176353 |\n",
      "|    clip_fraction        | 0.0739       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 5380         |\n",
      "|    policy_gradient_loss | -0.00745     |\n",
      "|    value_loss           | 5.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=983216, episode_reward=92.96 +/- 9.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 93           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 983216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018176353 |\n",
      "|    clip_fraction        | 0.0739       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 5380         |\n",
      "|    policy_gradient_loss | -0.00745     |\n",
      "|    value_loss           | 5.02         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=984216, episode_reward=78.50 +/- 5.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984216   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=984216, episode_reward=78.50 +/- 5.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 78.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985216, episode_reward=77.55 +/- 8.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985216, episode_reward=77.55 +/- 8.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986216, episode_reward=75.23 +/- 3.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986216, episode_reward=75.23 +/- 3.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 75.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 43.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 987136   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 43.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3418     |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 987136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987216, episode_reward=85.51 +/- 4.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 85.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 987216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022020047 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.8          |\n",
      "|    n_updates            | 5400         |\n",
      "|    policy_gradient_loss | -0.00771     |\n",
      "|    value_loss           | 5.09         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=987216, episode_reward=85.51 +/- 4.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 85.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 987216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022020047 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.8          |\n",
      "|    n_updates            | 5400         |\n",
      "|    policy_gradient_loss | -0.00771     |\n",
      "|    value_loss           | 5.09         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=988216, episode_reward=74.92 +/- 5.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988216, episode_reward=74.92 +/- 5.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989216, episode_reward=82.53 +/- 11.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989216, episode_reward=82.53 +/- 11.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 82.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990216, episode_reward=84.07 +/- 11.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990216, episode_reward=84.07 +/- 11.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991216, episode_reward=74.52 +/- 5.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991216, episode_reward=74.52 +/- 5.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992216, episode_reward=74.81 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 74.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 992216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014089374 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 5420         |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 5.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=992216, episode_reward=74.81 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 74.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 992216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014089374 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 5420         |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 5.3          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=993216, episode_reward=80.69 +/- 8.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993216, episode_reward=80.69 +/- 8.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994216, episode_reward=76.08 +/- 5.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994216, episode_reward=76.08 +/- 5.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 76.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995216, episode_reward=80.83 +/- 4.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995216, episode_reward=80.83 +/- 4.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 80.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996216, episode_reward=80.30 +/- 8.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 80.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 996216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017606036 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.48         |\n",
      "|    n_updates            | 5440         |\n",
      "|    policy_gradient_loss | -0.00796     |\n",
      "|    value_loss           | 5.38         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=996216, episode_reward=80.30 +/- 8.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 80.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 996216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017606036 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.48         |\n",
      "|    n_updates            | 5440         |\n",
      "|    policy_gradient_loss | -0.00796     |\n",
      "|    value_loss           | 5.38         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=997216, episode_reward=77.36 +/- 4.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997216, episode_reward=77.36 +/- 4.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 77.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998216, episode_reward=74.15 +/- 2.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998216, episode_reward=74.15 +/- 2.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999216, episode_reward=81.67 +/- 10.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999216, episode_reward=81.67 +/- 10.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999216   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 45.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000216, episode_reward=81.03 +/- 7.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 81           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1000216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018291221 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.41         |\n",
      "|    n_updates            | 5460         |\n",
      "|    policy_gradient_loss | -0.00782     |\n",
      "|    value_loss           | 5.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1000216, episode_reward=81.03 +/- 7.80\n",
      "Episode length: 20.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20           |\n",
      "|    mean_reward          | 81           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1000216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018291221 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.41         |\n",
      "|    n_updates            | 5460         |\n",
      "|    policy_gradient_loss | -0.00782     |\n",
      "|    value_loss           | 5.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1001216, episode_reward=72.08 +/- 0.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1001216, episode_reward=72.08 +/- 0.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1002216, episode_reward=79.62 +/- 11.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1002216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1002216, episode_reward=79.62 +/- 11.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 79.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1002216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1003216, episode_reward=74.57 +/- 1.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1003216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1003216, episode_reward=74.57 +/- 1.92\n",
      "Episode length: 20.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 74.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1003216  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 1003520  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 44.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3417     |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 1003520  |\n",
      "---------------------------------\n",
      "Training completed!\n",
      "Model saved as 'quantum_network_ppo_final'\n",
      "\n",
      "Testing the trained model...\n",
      "Episode 1: Reward = 31.905\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 2: Reward = 33.312\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 3: Reward = 33.077\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 4: Reward = 31.888\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 5: Reward = 29.807\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "\n",
      "Average reward over 5 test episodes: 31.998\n",
      "Training completed!\n",
      "Model saved as 'quantum_network_ppo_final'\n",
      "\n",
      "Testing the trained model...\n",
      "Episode 1: Reward = 31.905\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 2: Reward = 33.312\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 3: Reward = 33.077\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 4: Reward = 31.888\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "Episode 5: Reward = 29.807\n",
      "  Actions = [array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5), array(5)]\n",
      "  Details = ['R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5', 'R0P5']\n",
      "\n",
      "Average reward over 5 test episodes: 31.998\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"Training for 10,000 timesteps (adjust as needed)\")\n",
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=1_000_000,\n",
    "    callback=eval_callback,\n",
    "    tb_log_name=\"PPO_QuantumNetwork\",\n",
    "    #progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"protocol_picker_agents/quantum_network_ppo_final\")\n",
    "print(\"Model saved as 'quantum_network_ppo_final'\")\n",
    "\n",
    "# Load and test the trained model\n",
    "print(\"\\nTesting the trained model...\")\n",
    "trained_model = PPO.load(\"protocol_picker_agents/quantum_network_ppo_final\")\n",
    "\n",
    "# Test on a few episodes with detailed action analysis\n",
    "test_env = QuantumNetworkEnv(max_steps=10)\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, info = test_env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "    episode_actions = []\n",
    "    episode_details = []\n",
    "    \n",
    "    for step in range(10):\n",
    "        action, _states = trained_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_actions.append(action)\n",
    "        \n",
    "        # Decode action for detailed analysis\n",
    "        root, protocol = test_env.decode_action(action)\n",
    "        episode_details.append(f\"R{root}P{protocol}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.3f}\")\n",
    "    print(f\"  Actions = {episode_actions}\")\n",
    "    print(f\"  Details = {episode_details}\")\n",
    "\n",
    "average_reward = np.mean(total_rewards)\n",
    "print(f\"\\nAverage reward over 5 test episodes: {average_reward:.3f}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eba8fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=./ppo_protocol_picker_logs/ --port=6006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af830224",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d877ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trained RL Agent and Simulating on theta=[0.1, 0.2, 0.3]\n",
      "======================================================================\n",
      "Successfully loaded trained PPO model!\n",
      "\n",
      "Quantum Network Configuration:\n",
      "   Theta values: [0.1, 0.2, 0.3]\n",
      "   Max steps: 10\n",
      "   Action space: 18 (3 roots × 6 protocols)\n",
      "\n",
      " Starting Simulation...\n",
      "--------------------------------------------------\n",
      "Initial State:\n",
      "  Theta: [0.1, 0.2, 0.3]\n",
      "  Fisher Matrix Diagonal: [0. 0. 0.]\n",
      "  Fisher Matrix Trace: 0.000\n",
      "  Observation Shape: (12,)\n",
      "\n",
      "Step 1:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 2.4941\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [11.111, 0.000, 0.000]\n",
      "  Total Reward: 2.4941\n",
      "\n",
      "Step 2:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 3.1451\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [22.222, 0.000, 0.000]\n",
      "  Total Reward: 5.6392\n",
      "\n",
      "Step 3:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 3.5361\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [33.333, 0.000, 0.000]\n",
      "  Total Reward: 9.1753\n",
      "\n",
      "Step 4:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 3.8165\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [44.444, 0.000, 0.000]\n",
      "  Total Reward: 12.9918\n",
      "\n",
      "Step 5:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 4.0352\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [55.556, 0.000, 0.000]\n",
      "  Total Reward: 17.0271\n",
      "\n",
      "Step 6:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 4.2146\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [66.667, 0.000, 0.000]\n",
      "  Total Reward: 21.2417\n",
      "\n",
      "Step 7:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 4.3666\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [77.778, 0.000, 0.000]\n",
      "  Total Reward: 25.6083\n",
      "\n",
      "Step 8:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 4.4986\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [88.889, 0.000, 0.000]\n",
      "  Total Reward: 30.1069\n",
      "\n",
      "Step 9:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 4.6151\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [100.000, 0.000, 0.000]\n",
      "  Total Reward: 34.7220\n",
      "\n",
      "Step 10:\n",
      "  Action: 5 → Root 0, Protocol 5\n",
      "  Reward: 4.7195\n",
      "  Fisher Contribution Diagonal: [11.111, 0.000, 0.000]\n",
      "  Cumulative Fisher Diagonal:   [111.111, 0.000, 0.000]\n",
      "  Total Reward: 39.4415\n",
      "\n",
      "SIMULATION SUMMARY\n",
      "==================================================\n",
      "Final Total Reward: 39.4415\n",
      "Final Fisher Matrix Diagonal: [111.111, 0.000, 0.000]\n",
      "Final Fisher Matrix Trace: 111.111\n",
      "Mean Fisher Information: 37.037\n",
      "Fisher Info Variance: 2743.485\n",
      "Coefficient of Variation: 1.414\n",
      "\n",
      "Agent Strategy Analysis:\n",
      "Root selection frequency:\n",
      "  Root 0: 10/10 times (100%)\n",
      "  Root 1: 0/10 times (0%)\n",
      "  Root 2: 0/10 times (0%)\n",
      "Protocol selection frequency:\n",
      "  Protocol 0: 0/10 times (0%)\n",
      "  Protocol 1: 0/10 times (0%)\n",
      "  Protocol 2: 0/10 times (0%)\n",
      "  Protocol 3: 0/10 times (0%)\n",
      "  Protocol 4: 0/10 times (0%)\n",
      "  Protocol 5: 10/10 times (100%)\n",
      "\n",
      "Root-Protocol combinations used:\n",
      "  R0P5: 10 times\n",
      "\n",
      "✓ Simulation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load and simulate trained RL agent on network with theta=[0.1, 0.2, 0.3]\n",
    "\n",
    "class FixedThetaQuantumEnv(QuantumNetworkEnv):\n",
    "    \"\"\"\n",
    "    Special environment with fixed theta values for testing\n",
    "    \"\"\"\n",
    "    def __init__(self, theta_values, max_steps=10):\n",
    "        super().__init__(max_steps=max_steps)\n",
    "        self.fixed_theta = theta_values\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset with fixed theta values\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.theta = self.fixed_theta.copy()  # Use fixed theta values\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "print(\"Loading Trained RL Agent and Simulating on theta=[0.1, 0.2, 0.3]\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the trained model\n",
    "try:\n",
    "    trained_model = PPO.load(\"protocol_picker_agents/quantum_network_ppo_final\")\n",
    "    print(\"Successfully loaded trained PPO model!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure you've trained and saved the model first.\")\n",
    "    trained_model = None\n",
    "\n",
    "if trained_model is not None:\n",
    "    # Create environment with fixed theta values\n",
    "    test_theta = [0.1, 0.2, 0.3]\n",
    "    sim_env = FixedThetaQuantumEnv(theta_values=test_theta, max_steps=10)\n",
    "    \n",
    "    print(f\"\\nQuantum Network Configuration:\")\n",
    "    print(f\"   Theta values: {test_theta}\")\n",
    "    print(f\"   Max steps: 10\")\n",
    "    print(f\"   Action space: 18 (3 roots × 6 protocols)\")\n",
    "    \n",
    "    # Run simulation\n",
    "    print(f\"\\n Starting Simulation...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    obs, info = sim_env.reset(seed=42)\n",
    "    episode_data = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Initial State:\")\n",
    "    print(f\"  Theta: {sim_env.theta}\")\n",
    "    print(f\"  Fisher Matrix Diagonal: {np.diag(sim_env.fisher_matrix)}\")\n",
    "    print(f\"  Fisher Matrix Trace: {np.trace(sim_env.fisher_matrix):.3f}\")\n",
    "    print(f\"  Observation Shape: {obs.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # Run 10 steps\n",
    "    for step in range(10):\n",
    "        # Get action from trained agent\n",
    "        action, _states = trained_model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Decode action\n",
    "        root, protocol = sim_env.decode_action(action)\n",
    "        \n",
    "        # Take step\n",
    "        obs, reward, terminated, truncated, info = sim_env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Store step data\n",
    "        step_data = {\n",
    "            'step': step + 1,\n",
    "            'action': action,\n",
    "            'root': root,\n",
    "            'protocol': protocol,\n",
    "            'reward': reward,\n",
    "            'fisher_contribution': info['fisher_contribution'],\n",
    "            'fisher_matrix': info['fisher_matrix'],\n",
    "            'total_reward': total_reward\n",
    "        }\n",
    "        episode_data.append(step_data)\n",
    "        \n",
    "        # Print step details\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Action: {action} → Root {root}, Protocol {protocol}\")\n",
    "        print(f\"  Reward: {reward:.4f}\")\n",
    "        print(f\"  Fisher Contribution Diagonal: [{np.diag(info['fisher_contribution'])[0]:.3f}, {np.diag(info['fisher_contribution'])[1]:.3f}, {np.diag(info['fisher_contribution'])[2]:.3f}]\")\n",
    "        print(f\"  Cumulative Fisher Diagonal:   [{np.diag(info['fisher_matrix'])[0]:.3f}, {np.diag(info['fisher_matrix'])[1]:.3f}, {np.diag(info['fisher_matrix'])[2]:.3f}]\")\n",
    "        print(f\"  Total Reward: {total_reward:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Final Analysis\n",
    "    print(\"SIMULATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Final Total Reward: {total_reward:.4f}\")\n",
    "    print(f\"Final Fisher Matrix Diagonal: [{np.diag(sim_env.fisher_matrix)[0]:.3f}, {np.diag(sim_env.fisher_matrix)[1]:.3f}, {np.diag(sim_env.fisher_matrix)[2]:.3f}]\")\n",
    "    print(f\"Final Fisher Matrix Trace: {np.trace(sim_env.fisher_matrix):.3f}\")\n",
    "    \n",
    "    # Calculate balance metrics\n",
    "    final_fisher = np.diag(sim_env.fisher_matrix)\n",
    "    mean_fisher = np.mean(final_fisher)\n",
    "    variance_fisher = np.var(final_fisher)\n",
    "    cv = np.sqrt(variance_fisher) / mean_fisher if mean_fisher > 0 else 0\n",
    "    \n",
    "    print(f\"Mean Fisher Information: {mean_fisher:.3f}\")\n",
    "    print(f\"Fisher Info Variance: {variance_fisher:.3f}\")\n",
    "    print(f\"Coefficient of Variation: {cv:.3f}\")\n",
    "    \n",
    "    # Action selection analysis\n",
    "    roots_used = [data['root'] for data in episode_data]\n",
    "    protocols_used = [data['protocol'] for data in episode_data]\n",
    "    \n",
    "    print(f\"\\nAgent Strategy Analysis:\")\n",
    "    print(f\"Root selection frequency:\")\n",
    "    for root in range(3):\n",
    "        count = roots_used.count(root)\n",
    "        print(f\"  Root {root}: {count}/10 times ({count*10}%)\")\n",
    "    \n",
    "    print(f\"Protocol selection frequency:\")\n",
    "    for protocol in range(6):\n",
    "        count = protocols_used.count(protocol)\n",
    "        print(f\"  Protocol {protocol}: {count}/10 times ({count*10}%)\")\n",
    "    \n",
    "    # Most frequent combinations\n",
    "    combinations = [(data['root'], data['protocol']) for data in episode_data]\n",
    "    unique_combinations = list(set(combinations))\n",
    "    print(f\"\\nRoot-Protocol combinations used:\")\n",
    "    for combo in unique_combinations:\n",
    "        count = combinations.count(combo)\n",
    "        print(f\"  R{combo[0]}P{combo[1]}: {count} times\")\n",
    "    \n",
    "    sim_env.close()\n",
    "    print(\"\\n✓ Simulation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41175ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DETAILED ANALYSIS\n",
      "==================================================\n",
      "Balance Score: 0.414 (closer to 1.0 = better balanced)\n",
      "Efficiency: 11.111 Fisher Info per step\n",
      "Dominant Root: 0 (used 10/10 times)\n",
      "Dominant Protocol: 5 (used 10/10 times)\n",
      "\n",
      "Overall Performance: Fair\n",
      "The agent could improve balance or efficiency\n"
     ]
    }
   ],
   "source": [
    "# Visualize the simulation results\n",
    "\n",
    "if trained_model is not None and 'episode_data' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('RL Agent Simulation Results: θ = [0.1, 0.2, 0.3]', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    steps = [data['step'] for data in episode_data]\n",
    "    rewards = [data['reward'] for data in episode_data]\n",
    "    cumulative_rewards = [data['total_reward'] for data in episode_data]\n",
    "    fisher_0 = [np.diag(data['fisher_matrix'])[0] for data in episode_data]\n",
    "    fisher_1 = [np.diag(data['fisher_matrix'])[1] for data in episode_data]\n",
    "    fisher_2 = [np.diag(data['fisher_matrix'])[2] for data in episode_data]\n",
    "    roots = [data['root'] for data in episode_data]\n",
    "    protocols = [data['protocol'] for data in episode_data]\n",
    "    \n",
    "    # Plot 1: Fisher Information Evolution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(steps, fisher_0, 'o-', label='θ₀ = 0.1', color='blue', linewidth=2, markersize=6)\n",
    "    ax1.plot(steps, fisher_1, 's-', label='θ₁ = 0.2', color='red', linewidth=2, markersize=6)\n",
    "    ax1.plot(steps, fisher_2, '^-', label='θ₂ = 0.3', color='green', linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Cumulative Fisher Information')\n",
    "    ax1.set_title('Fisher Information Evolution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Action Selection Pattern\n",
    "    ax2 = axes[0, 1]\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'p']\n",
    "    \n",
    "    for i, (root, protocol) in enumerate(zip(roots, protocols)):\n",
    "        ax2.scatter(i+1, root, c=colors[root], s=200, marker=markers[protocol], \n",
    "                   alpha=0.8, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Root Selected')\n",
    "    ax2.set_title('Root and Protocol Selection Pattern')\n",
    "    ax2.set_yticks([0, 1, 2])\n",
    "    ax2.set_xticks(range(1, 11))\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    protocol_labels = ['Multicast', 'RI', 'IE X basis', 'IE Z basis', 'BF Z basis', 'BF X basis']\n",
    "    # Create custom legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    root_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], \n",
    "                         markersize=10, label=f'Root {i}') for i in range(3)]\n",
    "    protocol_legend = [Line2D([0], [0], marker=markers[i], color='w', markerfacecolor='gray', \n",
    "                             markersize=10, label=protocol_labels[i]) for i in range(6)]\n",
    "    ax2.legend(handles=root_legend + protocol_legend, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Plot 3: Reward Evolution\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.bar(steps, rewards, alpha=0.7, color='purple', label='Step Rewards')\n",
    "    ax3.plot(steps, cumulative_rewards, 'ro-', linewidth=2, markersize=6, label='Cumulative Reward')\n",
    "    ax3.set_xlabel('Step')\n",
    "    ax3.set_ylabel('Reward')\n",
    "    ax3.set_title('Reward Evolution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Fisher Information Contribution per Step\n",
    "    ax4 = axes[1, 1]\n",
    "    fisher_contrib_0 = [np.diag(data['fisher_contribution'])[0] for data in episode_data]\n",
    "    fisher_contrib_1 = [np.diag(data['fisher_contribution'])[1] for data in episode_data]\n",
    "    fisher_contrib_2 = [np.diag(data['fisher_contribution'])[2] for data in episode_data]\n",
    "    \n",
    "    width = 0.25\n",
    "    x = np.array(steps)\n",
    "    ax4.bar(x - width, fisher_contrib_0, width, label='θ₀ = 0.1', color='blue', alpha=0.7)\n",
    "    ax4.bar(x, fisher_contrib_1, width, label='θ₁ = 0.2', color='red', alpha=0.7)\n",
    "    ax4.bar(x + width, fisher_contrib_2, width, label='θ₂ = 0.3', color='green', alpha=0.7)\n",
    "    \n",
    "    ax4.set_xlabel('Step')\n",
    "    ax4.set_ylabel('Fisher Info Contribution')\n",
    "    ax4.set_title('Fisher Information Contribution per Step')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\nDETAILED ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Balance assessment\n",
    "    balance_score = 1 / (1 + cv)  # Higher score = better balance\n",
    "    print(f\"Balance Score: {balance_score:.3f} (closer to 1.0 = better balanced)\")\n",
    "    \n",
    "    # Efficiency assessment\n",
    "    total_fisher = np.sum(final_fisher)\n",
    "    efficiency = total_fisher / 10  # Fisher info per step\n",
    "    print(f\"Efficiency: {efficiency:.3f} Fisher Info per step\")\n",
    "    \n",
    "    # Strategy assessment\n",
    "    dominant_root = max(set(roots), key=roots.count)\n",
    "    dominant_protocol = max(set(protocols), key=protocols.count)\n",
    "    print(f\"Dominant Root: {dominant_root} (used {roots.count(dominant_root)}/10 times)\")\n",
    "    print(f\"Dominant Protocol: {dominant_protocol} (used {protocols.count(dominant_protocol)}/10 times)\")\n",
    "    \n",
    "    # Performance rating\n",
    "    if balance_score > 0.8 and efficiency > 5.0:\n",
    "        rating = \"Excellent\"\n",
    "    elif balance_score > 0.6 and efficiency > 3.0:\n",
    "        rating = \"Good\"\n",
    "    elif balance_score > 0.4 and efficiency > 1.0:\n",
    "        rating = \"Fair\"\n",
    "    else:\n",
    "        rating = \"Poor\"\n",
    "    \n",
    "    print(f\"\\nOverall Performance: {rating}\")\n",
    "    print(f\"The agent {'achieved good balance and efficiency' if 'Excellent' in rating or 'Good' in rating else 'could improve balance or efficiency'}\")\n",
    "\n",
    "else:\n",
    "    print(\"No simulation data available. Please run the simulation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ea17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
