{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e33d66",
   "metadata": {},
   "source": [
    "# Train Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ceaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable-Baselines3 and Gymnasium Implementation\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "\n",
    "# Import our functions\n",
    "from main import updated_fisher_information, reward_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a93f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumNetworkEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Gymnasium Environment for Quantum Network Protocol Selection\n",
    "    \n",
    "    The agent receives theta values and current Fisher information, then selects\n",
    "    protocols to maximize cumulative reward over multiple steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 4}\n",
    "    \n",
    "    def __init__(self, max_steps=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.render_mode = render_mode\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Action space: Combined root (3 options) and protocol (6 options) selection\n",
    "        # Total actions = 3 roots × 6 protocols = 18 possible actions\n",
    "        # Action encoding: action = root * 6 + protocol\n",
    "        # Where root ∈ {0, 1, 2} and protocol ∈ {0, 1, 2, 3, 4, 5}\n",
    "        self.action_space = spaces.Discrete(18)\n",
    "        \n",
    "        # Observation space: [theta0, theta1, theta2, fisher0, fisher1, fisher2]\n",
    "        # theta values are in (0.1, 0.9), Fisher info can be large positive values\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.1, 0.1, 0.1, 0.0, 0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([0.9, 0.9, 0.9, 1000.0, 1000.0, 1000.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.theta = None\n",
    "        self.fisher_info = None\n",
    "        self.total_reward = 0\n",
    "        self.episode_history = []\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.total_reward = 0\n",
    "        self.episode_history = []\n",
    "        \n",
    "        # Initialize theta values randomly\n",
    "        self.theta = [np.random.uniform(0.05, 0.45) for _ in range(3)]\n",
    "        \n",
    "        # Initialize Fisher information to zeros\n",
    "        self.fisher_info = [0.0, 0.0, 0.0]\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        if not self.action_space.contains(action):\n",
    "            raise ValueError(f\"Invalid action: {action}. Must be in {self.action_space}\")\n",
    "        \n",
    "        # Decode the combined action into root and protocol\n",
    "        # action = root * 6 + protocol\n",
    "        root = action // 6  # Integer division to get root (0, 1, or 2)\n",
    "        protocol = action % 6  # Modulo to get protocol (0, 1, 2, 3, 4, 5)\n",
    "        \n",
    "        # Store old fisher info for reward calculation\n",
    "        old_fisher_info = self.fisher_info.copy()\n",
    "        \n",
    "        try:\n",
    "            # Update Fisher information using the selected root and protocol\n",
    "            new_fisher_contribution = updated_fisher_information(root, protocol, self.theta)\n",
    "            \n",
    "            # Add new contribution to existing Fisher information\n",
    "            self.fisher_info = [old + new for old, new in zip(self.fisher_info, new_fisher_contribution)]\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = reward_function(old_fisher_info, new_fisher_contribution)\n",
    "            \n",
    "        except ZeroDivisionError:\n",
    "            # Handle edge case where theta values cause division by zero\n",
    "            reward = -10.0  # Penalty for invalid state\n",
    "            new_fisher_contribution = [0.0, 0.0, 0.0]\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        terminated = self.step_count >= self.max_steps\n",
    "        truncated = False  # We don't have time limits beyond max_steps\n",
    "        \n",
    "        # Add small noise to theta values to simulate environment changes\n",
    "        theta_noise = 0.01\n",
    "        self.theta = [\n",
    "            np.clip(t + np.random.uniform(-theta_noise, theta_noise), 0.1, 0.9)\n",
    "            for t in self.theta\n",
    "        ]\n",
    "        \n",
    "        # Store step information for analysis\n",
    "        step_info = {\n",
    "            'step': self.step_count,\n",
    "            'action': action,\n",
    "            'root': root,\n",
    "            'protocol': protocol,\n",
    "            'reward': reward,\n",
    "            'fisher_contribution': new_fisher_contribution.copy(),\n",
    "            'cumulative_fisher': self.fisher_info.copy(),\n",
    "            'theta': self.theta.copy()\n",
    "        }\n",
    "        self.episode_history.append(step_info)\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        info.update(step_info)\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation: [theta0, theta1, theta2, fisher0, fisher1, fisher2]\"\"\"\n",
    "        obs = np.array(self.theta + self.fisher_info, dtype=np.float32)\n",
    "        # Clip to ensure it's within observation space bounds\n",
    "        return np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"Get additional information about the current state\"\"\"\n",
    "        return {\n",
    "            'step_count': self.step_count,\n",
    "            'total_reward': self.total_reward,\n",
    "            'theta': self.theta.copy(),\n",
    "            'fisher_info': self.fisher_info.copy()\n",
    "        }\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        \"\"\"Decode combined action into root and protocol\"\"\"\n",
    "        root = action // 6\n",
    "        protocol = action % 6\n",
    "        return root, protocol\n",
    "    \n",
    "    def encode_action(self, root, protocol):\n",
    "        \"\"\"Encode root and protocol into combined action\"\"\"\n",
    "        return root * 6 + protocol\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment (optional)\"\"\"\n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"Step: {self.step_count}/{self.max_steps}\")\n",
    "            print(f\"Theta: {[f'{x:.3f}' for x in self.theta]}\")\n",
    "            print(f\"Fisher Info: {[f'{x:.3f}' for x in self.fisher_info]}\")\n",
    "            print(f\"Total Reward: {self.total_reward:.3f}\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up environment\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d25d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the environment with updated action space\n",
    "print(\"Creating Quantum Network Environment with Root + Protocol Selection...\")\n",
    "env = QuantumNetworkEnv(max_steps=10)\n",
    "\n",
    "# Check if the environment follows Gymnasium API\n",
    "print(\"Checking environment...\")\n",
    "try:\n",
    "    check_env(env, warn=True)\n",
    "    print(\"✓ Environment check passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Environment check failed: {e}\")\n",
    "\n",
    "# Test the environment\n",
    "print(\"\\nTesting environment reset...\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Initial observation: {obs}\")\n",
    "print(f\"Action space: {env.action_space} (18 total actions)\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Demonstrate action encoding\n",
    "print(\"\\nAction Encoding Examples:\")\n",
    "print(\"Action = Root * 6 + Protocol\")\n",
    "for root in range(3):\n",
    "    for protocol in range(6):\n",
    "        encoded_action = env.encode_action(root, protocol)\n",
    "        decoded_root, decoded_protocol = env.decode_action(encoded_action)\n",
    "        print(f\"Root {root}, Protocol {protocol} → Action {encoded_action}\")\n",
    "print()\n",
    "\n",
    "# Test a few random steps with detailed output\n",
    "print(\"Testing random steps with root and protocol details...\")\n",
    "for i in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    root, protocol = env.decode_action(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action} (Root={root}, Protocol={protocol}), Reward={reward:.3f}, Done={terminated}\")\n",
    "    \n",
    "env.close()\n",
    "print(\"Environment test completed successfully! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a PPO agent using Stable-Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "\n",
    "print(\"Setting up training environment...\")\n",
    "\n",
    "# Create training and evaluation environments\n",
    "train_env = QuantumNetworkEnv(max_steps=20)\n",
    "eval_env = QuantumNetworkEnv(max_steps=20)\n",
    "\n",
    "# Wrap environments with Monitor for logging\n",
    "train_env = Monitor(train_env)\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Create the PPO model\n",
    "print(\"Creating PPO model...\")\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Multi-layer perceptron policy\n",
    "    train_env,\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,  # Encourage exploration\n",
    "    device=\"cpu\",  # Use CPU for compatibility\n",
    "    tensorboard_log=\"./ppo_quantum_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Policy architecture: {model.policy}\")\n",
    "\n",
    "# Create evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_quantum_model/\",\n",
    "    log_path=\"./eval_logs/\",\n",
    "    eval_freq=1000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training setup completed! Ready to train the agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785260c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"Training for 50,000 timesteps (adjust as needed)\")\n",
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=50000,\n",
    "    callback=eval_callback,\n",
    "    tb_log_name=\"PPO_QuantumNetwork\",\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"quantum_network_ppo_final\")\n",
    "print(\"Model saved as 'quantum_network_ppo_final'\")\n",
    "\n",
    "# Load and test the trained model\n",
    "print(\"\\nTesting the trained model...\")\n",
    "trained_model = PPO.load(\"quantum_network_ppo_final\")\n",
    "\n",
    "# Test on a few episodes with detailed action analysis\n",
    "test_env = QuantumNetworkEnv(max_steps=10)\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, info = test_env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "    episode_actions = []\n",
    "    episode_details = []\n",
    "    \n",
    "    for step in range(10):\n",
    "        action, _states = trained_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_actions.append(action)\n",
    "        \n",
    "        # Decode action for detailed analysis\n",
    "        root, protocol = test_env.decode_action(action)\n",
    "        episode_details.append(f\"R{root}P{protocol}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.3f}\")\n",
    "    print(f\"  Actions = {episode_actions}\")\n",
    "    print(f\"  Details = {episode_details}\")\n",
    "\n",
    "average_reward = np.mean(total_rewards)\n",
    "print(f\"\\nAverage reward over 5 test episodes: {average_reward:.3f}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and Visualization of Agent Performance\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_agent_performance(model, num_episodes=100):\n",
    "    \"\"\"Analyze the trained agent's root and protocol selection patterns.\"\"\"\n",
    "    \n",
    "    env = QuantumNetworkEnv(max_steps=10)\n",
    "    all_actions = []\n",
    "    all_roots = []\n",
    "    all_protocols = []\n",
    "    all_rewards = []\n",
    "    all_theta_values = []\n",
    "    all_fisher_values = []\n",
    "    \n",
    "    print(f\"Analyzing agent performance over {num_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset(seed=episode)\n",
    "        episode_actions = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Store initial theta and fisher values\n",
    "        theta_values = obs[:3].tolist()\n",
    "        fisher_values = obs[3:].tolist()\n",
    "        \n",
    "        for step in range(10):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Decode action\n",
    "            root, protocol = env.decode_action(action)\n",
    "            \n",
    "            episode_actions.append(action)\n",
    "            all_actions.append(action)\n",
    "            all_roots.append(root)\n",
    "            all_protocols.append(protocol)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        all_theta_values.append(theta_values)\n",
    "        all_fisher_values.append(fisher_values)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Analysis\n",
    "    action_counts = Counter(all_actions)\n",
    "    root_counts = Counter(all_roots)\n",
    "    protocol_counts = Counter(all_protocols)\n",
    "    avg_reward = np.mean(all_rewards)\n",
    "    \n",
    "    print(f\"\\nPerformance Analysis:\")\n",
    "    print(f\"Average reward per episode: {avg_reward:.3f}\")\n",
    "    print(f\"Root selection frequency:\")\n",
    "    for root in range(3):\n",
    "        count = root_counts.get(root, 0)\n",
    "        percentage = (count / len(all_roots)) * 100\n",
    "        print(f\"  Root {root}: {count} times ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"Protocol selection frequency:\")\n",
    "    for protocol in range(6):\n",
    "        count = protocol_counts.get(protocol, 0)\n",
    "        percentage = (count / len(all_protocols)) * 100\n",
    "        print(f\"  Protocol {protocol}: {count} times ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Root and Protocol selection frequency\n",
    "    # Create a combined plot\n",
    "    ax1_twin = ax1.twinx()\n",
    "    \n",
    "    roots = list(range(3))\n",
    "    protocols = list(range(6))\n",
    "    root_frequencies = [root_counts.get(r, 0) for r in roots]\n",
    "    protocol_frequencies = [protocol_counts.get(p, 0) for p in protocols]\n",
    "    \n",
    "    bars1 = ax1.bar([r - 0.2 for r in roots], root_frequencies, width=0.4, \n",
    "                    color='skyblue', alpha=0.7, label='Root Selection')\n",
    "    bars2 = ax1_twin.bar(protocols, protocol_frequencies, width=0.4, \n",
    "                         color='lightcoral', alpha=0.7, label='Protocol Selection')\n",
    "    \n",
    "    ax1.set_xlabel('Root / Protocol ID')\n",
    "    ax1.set_ylabel('Root Selection Frequency', color='skyblue')\n",
    "    ax1_twin.set_ylabel('Protocol Selection Frequency', color='lightcoral')\n",
    "    ax1.set_title('Root and Protocol Selection Frequency')\n",
    "    ax1.set_xticks(range(6))\n",
    "    ax1.tick_params(axis='y', labelcolor='skyblue')\n",
    "    ax1_twin.tick_params(axis='y', labelcolor='lightcoral')\n",
    "    \n",
    "    # 2. Reward distribution\n",
    "    ax2.hist(all_rewards, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Episode Reward')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Episode Reward Distribution')\n",
    "    ax2.axvline(avg_reward, color='red', linestyle='--', label=f'Mean: {avg_reward:.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Theta values distribution\n",
    "    theta_flat = [theta for episode_thetas in all_theta_values for theta in episode_thetas]\n",
    "    ax3.hist(theta_flat, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xlabel('Theta Values')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Distribution of Theta Values Encountered')\n",
    "    \n",
    "    # 4. Fisher information distribution\n",
    "    fisher_flat = [fisher for episode_fishers in all_fisher_values for fisher in episode_fishers]\n",
    "    # Remove infinite values for plotting\n",
    "    fisher_finite = [f for f in fisher_flat if np.isfinite(f)]\n",
    "    if fisher_finite:\n",
    "        ax4.hist(fisher_finite, bins=20, color='purple', alpha=0.7, edgecolor='black')\n",
    "        ax4.set_xlabel('Fisher Information')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title('Distribution of Fisher Information Values')\n",
    "        ax4.set_yscale('log')  # Log scale due to wide range\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': avg_reward,\n",
    "        'action_counts': action_counts,\n",
    "        'root_counts': root_counts,\n",
    "        'protocol_counts': protocol_counts,\n",
    "        'all_rewards': all_rewards\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Analyzing the trained agent...\")\n",
    "results = analyze_agent_performance(trained_model, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d029d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Understanding the New Action Space\n",
    "print(\"🔧 Action Space Modification Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n📊 Action Encoding System:\")\n",
    "print(\"The agent now has full control over both ROOT and PROTOCOL selection!\")\n",
    "print(\"\\nTotal action space: 18 actions (3 roots × 6 protocols)\")\n",
    "print(\"Encoding: action = root * 6 + protocol\")\n",
    "print(\"Where:\")\n",
    "print(\"  • Root ∈ {0, 1, 2} (which node to measure)\")\n",
    "print(\"  • Protocol ∈ {0, 1, 2, 3, 4, 5} (which protocol to use)\")\n",
    "\n",
    "print(\"\\n📋 Action Mapping Table:\")\n",
    "print(\"Action | Root | Protocol | Description\")\n",
    "print(\"-------|------|----------|------------\")\n",
    "\n",
    "env = QuantumNetworkEnv()\n",
    "for action in range(18):\n",
    "    root, protocol = env.decode_action(action)\n",
    "    print(f\"  {action:2d}   |  {root}   |    {protocol}     | Root {root}, Protocol {protocol}\")\n",
    "\n",
    "print(\"\\n🎯 Benefits of This Modification:\")\n",
    "print(\"✓ Agent learns optimal root selection strategy\")\n",
    "print(\"✓ Full control over both measurement node and protocol\")\n",
    "print(\"✓ Can discover correlations between theta values and optimal roots\")\n",
    "print(\"✓ More sophisticated decision-making capability\")\n",
    "\n",
    "print(\"\\n🔄 Before vs After:\")\n",
    "print(\"Before: Agent selects protocol (6 options), root chosen randomly\")\n",
    "print(\"After:  Agent selects root AND protocol (18 total combinations)\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis: Compare Different RL Algorithms\n",
    "from stable_baselines3 import DQN, A2C\n",
    "\n",
    "def train_and_compare_algorithms():\n",
    "    \"\"\"Train multiple RL algorithms and compare their performance.\"\"\"\n",
    "    \n",
    "    algorithms = {\n",
    "        'PPO': PPO,\n",
    "        'DQN': DQN,\n",
    "        'A2C': A2C\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, algorithm_class in algorithms.items():\n",
    "        print(f\"\\nTraining {name} algorithm...\")\n",
    "        \n",
    "        # Create fresh environment\n",
    "        train_env = Monitor(QuantumNetworkEnv(max_steps=20))\n",
    "        \n",
    "        # Algorithm-specific parameters\n",
    "        if name == 'DQN':\n",
    "            model = algorithm_class(\n",
    "                \"MlpPolicy\",\n",
    "                train_env,\n",
    "                verbose=1,\n",
    "                learning_rate=1e-3,\n",
    "                buffer_size=10000,\n",
    "                learning_starts=1000,\n",
    "                batch_size=32,\n",
    "                tau=1.0,\n",
    "                gamma=0.99,\n",
    "                train_freq=4,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=1000,\n",
    "                exploration_fraction=0.1,\n",
    "                exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.05,\n",
    "                device=\"cpu\"\n",
    "            )\n",
    "        elif name == 'A2C':\n",
    "            model = algorithm_class(\n",
    "                \"MlpPolicy\",\n",
    "                train_env,\n",
    "                verbose=1,\n",
    "                learning_rate=7e-4,\n",
    "                n_steps=5,\n",
    "                gamma=0.99,\n",
    "                gae_lambda=1.0,\n",
    "                ent_coef=0.01,\n",
    "                vf_coef=0.25,\n",
    "                max_grad_norm=0.5,\n",
    "                device=\"cpu\"\n",
    "            )\n",
    "        else:  # PPO\n",
    "            model = algorithm_class(\n",
    "                \"MlpPolicy\",\n",
    "                train_env,\n",
    "                verbose=1,\n",
    "                learning_rate=3e-4,\n",
    "                n_steps=2048,\n",
    "                batch_size=64,\n",
    "                n_epochs=10,\n",
    "                gamma=0.99,\n",
    "                gae_lambda=0.95,\n",
    "                clip_range=0.2,\n",
    "                ent_coef=0.01,\n",
    "                device=\"cpu\"\n",
    "            )\n",
    "        \n",
    "        # Train the model (shorter training for comparison)\n",
    "        model.learn(total_timesteps=20000, progress_bar=True)\n",
    "        \n",
    "        # Save model\n",
    "        model.save(f\"quantum_network_{name.lower()}\")\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Test performance\n",
    "        test_env = QuantumNetworkEnv(max_steps=10)\n",
    "        episode_rewards = []\n",
    "        \n",
    "        for episode in range(20):\n",
    "            obs, info = test_env.reset(seed=episode)\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for step in range(10):\n",
    "                action, _states = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "        \n",
    "        test_env.close()\n",
    "        train_env.close()\n",
    "        \n",
    "        avg_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "        \n",
    "        results[name] = {\n",
    "            'avg_reward': avg_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'episode_rewards': episode_rewards\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - Average Reward: {avg_reward:.3f} ± {std_reward:.3f}\")\n",
    "    \n",
    "    return results, trained_models\n",
    "\n",
    "def plot_algorithm_comparison(results):\n",
    "    \"\"\"Plot comparison of different algorithms.\"\"\"\n",
    "    \n",
    "    algorithms = list(results.keys())\n",
    "    avg_rewards = [results[alg]['avg_reward'] for alg in algorithms]\n",
    "    std_rewards = [results[alg]['std_reward'] for alg in algorithms]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot of average rewards\n",
    "    bars = ax1.bar(algorithms, avg_rewards, yerr=std_rewards, \n",
    "                   capsize=5, color=['skyblue', 'lightgreen', 'orange'], alpha=0.7)\n",
    "    ax1.set_ylabel('Average Reward')\n",
    "    ax1.set_title('Algorithm Performance Comparison')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, avg, std in zip(bars, avg_rewards, std_rewards):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + std/2,\n",
    "                f'{avg:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Box plot of reward distributions\n",
    "    reward_distributions = [results[alg]['episode_rewards'] for alg in algorithms]\n",
    "    ax2.boxplot(reward_distributions, labels=algorithms)\n",
    "    ax2.set_ylabel('Episode Reward')\n",
    "    ax2.set_title('Reward Distribution by Algorithm')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run algorithm comparison\n",
    "print(\"Starting algorithm comparison...\")\n",
    "print(\"This will train PPO, DQN, and A2C algorithms and compare their performance.\")\n",
    "print(\"Training 3 algorithms for 20,000 timesteps each...\")\n",
    "\n",
    "comparison_results, comparison_models = train_and_compare_algorithms()\n",
    "plot_algorithm_comparison(comparison_results)\n",
    "\n",
    "print(\"\\nAlgorithm Comparison Summary:\")\n",
    "for alg, result in comparison_results.items():\n",
    "    print(f\"{alg}: {result['avg_reward']:.3f} ± {result['std_reward']:.3f}\")\n",
    "\n",
    "# Find best performing algorithm\n",
    "best_algorithm = max(comparison_results.keys(), \n",
    "                    key=lambda x: comparison_results[x]['avg_reward'])\n",
    "print(f\"\\nBest performing algorithm: {best_algorithm}\")\n",
    "print(f\"Best average reward: {comparison_results[best_algorithm]['avg_reward']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Usage Instructions\n",
    "\n",
    "print(\"🎉 Quantum Network RL Training Setup Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nWhat we've built:\")\n",
    "print(\"✓ Professional Gymnasium environment for quantum network protocol selection\")\n",
    "print(\"✓ Integration with Stable-Baselines3 for state-of-the-art RL algorithms\")\n",
    "print(\"✓ Training pipeline with evaluation callbacks and monitoring\")\n",
    "print(\"✓ Comprehensive analysis and visualization tools\")\n",
    "print(\"✓ Comparison framework for multiple RL algorithms (PPO, DQN, A2C)\")\n",
    "\n",
    "print(\"\\n📋 How to use this setup:\")\n",
    "print(\"1. Run the environment test cell to verify everything works\")\n",
    "print(\"2. Train your preferred algorithm (PPO recommended)\")\n",
    "print(\"3. Analyze the trained agent's performance with visualization\")\n",
    "print(\"4. Compare different algorithms if needed\")\n",
    "print(\"5. Use the best model for your quantum network protocol selection!\")\n",
    "\n",
    "print(\"\\n🔧 Key Features:\")\n",
    "print(\"• Input: 3 theta values + 3 Fisher information values\")\n",
    "print(\"• Output: Selection of 1 protocol from 6 available options\")\n",
    "print(\"• Reward: Sum of Fisher information values for optimal quantum sensing\")\n",
    "print(\"• Algorithms: PPO, DQN, A2C (easily extensible)\")\n",
    "\n",
    "print(\"\\n📁 Files created:\")\n",
    "print(\"• quantum_network_ppo_final.zip - Trained PPO model\")\n",
    "print(\"• quantum_network_dqn.zip - Trained DQN model\") \n",
    "print(\"• quantum_network_a2c.zip - Trained A2C model\")\n",
    "print(\"• Tensorboard logs in ./ppo_quantum_tensorboard/\")\n",
    "print(\"• Best model checkpoints in ./best_quantum_model/\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"• Experiment with different hyperparameters\")\n",
    "print(\"• Extend to multi-step protocol selection\")\n",
    "print(\"• Add more sophisticated reward functions\")\n",
    "print(\"• Integrate with real quantum hardware simulations\")\n",
    "\n",
    "print(\"\\nHappy quantum computing! 🔬⚛️\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
